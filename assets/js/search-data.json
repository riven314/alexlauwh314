{
  
    
        "post0": {
            "title": "Delight Your VSCode Experience by Auto-formatting (Python, Javascript, Rust)",
            "content": "Motivation . . When I was in a data science role, I didn‚Äôt give much attention to tools for code formatting. Ever since I transitted to a software engineer, I started appreciating the value of these tools. I find it even handy to automate them in an IDE. . The benefits are more than just saving time: . It saves you cognitive workload from manual code formatting, hence you can focus on things that are more important | It enforces uniform coding style with your teammates. Your team observes to the same coding style for free once they share the same configuration of a formatter. | I will share how I personally install and configure formatters in my VSCode, so that it can be automatically triggered when saving a script in Python, Javascript (Typescript) or Rust. . It‚Äôs just a few simple steps, but your experience will never be the same! Now I cannot part without auto-formatting in my daily coding routine. . . Note: You may wonder the differences between Linter and Formatter. Linter flags bugs and violation of best practices, while Formatter enforces a coding style on your code. Best practices could be related to code style, so formatter could help address a small part of best practices. I may open a separate blogpost about Linters in the future. Python . . Install Black Formatter in Extension Marketplace (shortcut is Command + Shift + X in Macbook) . . | Open a project in VSCode, navigate to ‚ÄúUser Settings‚Äù (shortcut is Command + , in Macbook), then ‚ÄúWorkspace‚Äù tab, then ‚ÄúOpen Settings (JSON)‚Äù button, as shown in the screenshot. Now you are in the editing page of settings.json, which configures VSCode‚Äôs functionalities specific to your project. . . | Paste the following code snippet in settings.json: . { &quot;[python]&quot;: { &quot;editor.defaultFormatter&quot;: &quot;ms-python.black-formatter&quot;, &quot;editor.formatOnSave&quot;: true } } . | Optional: Make a new file named pyproject.toml in your project folder. It customises the formatting rules for Black Formatter. For example, if I want the number of characters per line to be no more than 80, I can paste the following snippet in the file: . [tool.black] line-length = 80 . | Voila! Auto-formatting is ready to run for Python script! üî• . Here is a short demo of how it behaves when I save a Python file: . . . . Note: Besides Black Formatter, there are other Python formatters available in Extension Market. For example, Isort for importing sorting. Alternatively, these formatters comes as a package, so that you can apply them in command line. For example, Black Formatter is making use of Black package underneath the hood. Flake8 is another package for code formatting but it doesn‚Äôt have an extension in VSCode. Feel free to explore them on your own! Javascript (or Typescript) . . Install Prettier - Code formatter in Extension Marketplace . . | Navigate to the editing page of settings.json and paste the following code snippet: . { &quot;[javascript]&quot;: { &quot;editor.defaultFormatter&quot;: &quot;esbenp.prettier-vscode&quot;, &quot;editor.formatOnSave&quot;: true }, &quot;[typescript]&quot;: { &quot;editor.defaultFormatter&quot;: &quot;esbenp.prettier-vscode&quot;, &quot;editor.formatOnSave&quot;: true } } . | Optional: Make a new file named prettierrc.json in your project folder. It customises the formatting rules for Prettier. For example, if I want to enforce rules like single quote in string and double spaces in tab, I can paste the following snippet into the file: . { &quot;singleQuote&quot;: true, &quot;trailingComma&quot;: &quot;all&quot;, &quot;printWidth&quot;: 100, &quot;tabWidth&quot;: 2, &quot;useTabs&quot;: false } . | Now auto-formatting is ready for Javacsript! Here is a demo: . . Rust . . Install rust-analyzer in Extension Marketplace (Rust analyzer is not just a formatter. It supports many features such as error popup, datatype detection) . . | Navigate to the editing page of settings.json and paste the following code snippet: . { &quot;[rust]&quot;: { &quot;editor.defaultFormatter&quot;: &quot;rust-lang.rust-analyzer&quot;, &quot;editor.formatOnSave&quot;: true } } . | Optional: Customising rules in rust-analyzer requires more effort, you can refer to this documentation for details. Its default setting suffices in most use cases. . | Here is the final demo to showcase the auto-formatting in Rust script: . . Conclusion . . In this post, I have shown you: . how to install different formatters in VSCode Extension Marketplace | how to customise your formatters with its configuration file | how to configure the formatter to be applied upon file saving in VSCode | You can further explore the functionalities of each formatters and its integration with VSCode. . As an advanced setup, you can even apply multiple formatters on a script upong saving (e.g. applying Black, Flake8 and Isort formatters all in once). If there are enough of readers interested in this trick, I could write an additional blogpost for that! . References . . How To Enable Linting on Save with Visual Studio Code and ESLint, DigitalOcean | Linting and Formatting with ESLint in VS Code | Styling and Formatting Code - Made With ML | How to run cargo fmt on save in vscode? | Support multiple formatters for a single file ¬∑ Issue #142904 ¬∑ microsoft/vscode | Automatically Execute Bash Commands on Save in VS Code |",
            "url": "https://riven314.github.io/alexlauwh314/ide/productivity/2022/08/12/VSCode-Autoformat.html",
            "relUrl": "/ide/productivity/2022/08/12/VSCode-Autoformat.html",
            "date": " ‚Ä¢ Aug 12, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Notes on "Clean Code in Python" ‚Äî When to Apply Inheritance?",
            "content": "Motivation . . Inheritance is a pattern typically seen in Object Oriented Programming (OOP) languages such as Python, but you may see some articles (like this one) criticizing it. . This post continues to summarize my take-aways from ‚ÄúClean Code in Python‚Äù. In the book, the author explains the trade-off for using inheritance, and highlight a few scenarios appropriate for applying inheritance. . Trade-off for Using Inheritance . . While inheritance has its own benefit, we should be mindful of the trade-off for using it. . ‚úÖ PRO: Reduce Code Repetition . Inheritance reduces code duplication because any child classes could reuse the methods from its parent class. It is in line with DRY (Don‚Äôt Repeat Yourself) principle. . A code with minimal repetition is readable as it avoids redundant information showing up over and over again. It is also easier to maintain as you only need to do code change in one place. . ‚ùå CON: Higher Coupling . Having said that, such benefit comes at a price. Inheritance introduces dependency between parent class and its subclass. Such interdependence is called Coupling. . Such dependency makes the code harder to maintain because any change you made in one of these classes inevitably propagates to its dependent classes. Such propagation is called Ripple Effect. . When you have a gigantic hierarchy in your inheritance, even a tiny change in one class could bring unanticipated impacts on other modules. . ‚ùå CON: Lower Cohesion . Reusing methods from parent class sounds great! But what if you only need a small subset of them? . Inheritance renders the rest of the methods redundant, as if they should not belong to the class. . It signals your subclass has issue in terms of Cohension. It brings you technical debt because you have to take care of those unwanted methods. The situation is worse when some of the unwanted methods are public interfaces to the user. You may be aware which methods are unwanted, but your users are free to use any of them. So you have to deal it! . You gain code reusability, but you pay additional cost for maintaining these unwanted interfaces. . When NOT to Use Inheritance? . . DON‚ÄôT apply inheritance simply for the sake of reusing codes! . Just because we get a few magic methods from a base class is not justified to introduce an inheritance. Don‚Äôt overlook the higher coupling and lower cohesion that it adversely introduces, it could easily outweigh the benefit. . DON‚ÄôT apply inheritance when two objects are under ‚Äúhas-a‚Äù relationship! . For example, a company has departments and employees. Department object shouldn‚Äôt inherit company object, so does employee object. A better alternative to represent such relationship is Object Composition. . When to Use Inheritance? . . Inheritance should describe a ‚Äúis-a‚Äù relationship. Child class should be functionally the same as its parent. Child class is a variant of its parent class. In addition, child class should serve as a specialization. It extends or modify features from its parents to serve a specific domain. . Below I summarise 3 scenarios appropriate for inheritance that the book showcases. For each scenario I attach a few examples from open source code. . Scenario 1 . . Your parent class has captured the overall pipeline, but a few of its components depends on interfaces to be defined in child classes. It is easier to illustrate this by examples. . ‚ú® Example: BaseHTTPRequestHandler and SimpleHTTPRequestHandler . First example is extracted from the built-in http library. There is a module with helper functions for server handling. . In the module, BaseHTTPRequestHandler is a class for handling HTTP requests in a server. This class implements a number of methods that can run independently. For example: . parse_request for parsing a request | log_request for logging an accepted request | send_header for sending a header to buffer | . But notice how handle_one_request is defined. Pay attention to the middle: . class BaseHTTPRequestHandler(socketserver.StreamRequestHandler): #################################### ### omit the rest of the methods ### #################################### def handle_one_request(self): try: self.raw_requestline = self.rfile.readline(65537) if len(self.raw_requestline) &gt; 65536: self.requestline = &#39;&#39; self.request_version = &#39;&#39; self.command = &#39;&#39; self.send_error(HTTPStatus.REQUEST_URI_TOO_LONG) return if not self.raw_requestline: self.close_connection = True return if not self.parse_request(): # An error code has been sent, just exit return mname = &#39;do_&#39; + self.command if not hasattr(self, mname): self.send_error( HTTPStatus.NOT_IMPLEMENTED, &quot;Unsupported method (%r)&quot; % self.command) return method = getattr(self, mname) method() self.wfile.flush() #actually send the response if not already done. except TimeoutError as e: #a read or a write timed out. Discard this connection self.log_error(&quot;Request timed out: %r&quot;, e) self.close_connection = True return . BaseHTTPRequestHandler attempts to grab and then call the target method (i.e. method) whose name has a pattern of do_&lt;request type&gt; (examples of request type are GET and POST), but it doesn‚Äôt have any of these methods. Why is that? . It‚Äôs because BaseHTTPRequestHandler is designed to be inherited. Those methods are meant to be defined in its child class. Such setting enables child class to handle different request types in its specific context. . SimpleHTTPRequestHandler is one example. It is a HTTP request handler specialized in GET and HEAD request types. It handles these request types with a simple rule, defined in do_GET and do_HEAD methods: . class SimpleHTTPRequestHandler(BaseHTTPRequestHandler): #################################### ### omit the rest of the methods ### #################################### def do_GET(self): &quot;&quot;&quot;Serve a GET request.&quot;&quot;&quot; f = self.send_head() if f: try: self.copyfile(f, self.wfile) finally: f.close() def do_HEAD(self): &quot;&quot;&quot;Serve a HEAD request.&quot;&quot;&quot; f = self.send_head() if f: f.close() . ‚ú® Example: Callback and FetchPredsCallback . Another example is extracted from fastai library, a high level Deep Learning framework built on top of PyTorch. . It has an interesting concept called Callback ‚Äî an interface for user to flexibly intercept any phrase in a training loop and then inject customized procedures. It provide a list of phrases where you can intercept. For example, you can intercept at the beginning of training loop, or you can intercept the end of loss computation. You can read this documentation to learn more about it. . As the name suggests, Callback class (from this module) is responsible for such callback interface. . Notice a short excerpt of its implementation. Pay attention to the middle: . class Callback(Stateful,GetAttr): #################################### ### omit the rest of the methods ### #################################### def __call__(self, event_name): &quot;Call `self.{event_name}` if it&#39;s defined&quot; _run = (event_name not in _inner_loop or (self.run_train and getattr(self, &#39;training&#39;, True)) or (self.run_valid and not getattr(self, &#39;training&#39;, False))) res = None if self.run and _run: try: res = getattr(self, event_name, noop)() except (CancelBatchException, CancelBackwardException, CancelEpochException, CancelFitException, CancelStepException, CancelTrainException, CancelValidException): raise except Exception as e: e.args = [f&#39;Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`: n t{e.args[0]}&#39;] raise if event_name==&#39;after_fit&#39;: self.run=True #Reset self.run to True at each end of fit return res . Its __call__ method attempts to seek and then call the target method whose name is the value of event_name. The value of event_name representats the phrase that you want to intercept. For example, before_epoch represents the beginning of training loop and after_loss represents the end of loss computation. . However, Callback doesn‚Äôt have any of these methods provided. So Callback is meant to be inherited! . One example of its child class is FetchPredsCallback ‚Äì a callback specialized in storing model prediction of validation sets. The step is done at the end of validation stage, suggested by its method name after_validate: . class FetchPredsCallback(Callback): #################################### ### omit the rest of the methods ### #################################### def after_validate(self): &quot;Fetch predictions from `Learner` without `self.cbs` and `remove_on_fetch` callbacks&quot; to_rm = L(cb for cb in self.learn.cbs if getattr(cb, &#39;remove_on_fetch&#39;, False)) with self.learn.removed_cbs(to_rm + self.cbs) as learn: self.preds = learn.get_preds(ds_idx=self.ds_idx, dl=self.dl, with_input=self.with_input, with_decoded=self.with_decoded, inner=True, reorder=self. . Scenario 2 . . You want to enforce the same interfaces across class objects. You can make use of parent class as an abstract class. . An abstract class enforces a contract with its child classes. It declares interfaces without a need to implement them, but its child classes must implement them in order to be instantiated. . ‚ú® Example: abc Module . abc is a handy library to help you define abstract class. . Any class inherited from abc.ABC class is treated as an abstract class and can‚Äôt be instantiated. You can declare the ‚Äúcontract-binding‚Äù interfaces with abc.abstractmethod. See its documentation for more details. . Here I provide a simple example on how to use abc module to define abstract class: . import numpy as np from abc import ABC, abstractmethod from sklearn.linear_model import LinearRegression class ABCModel(ABC): @abstractmethod def train(self, X: np.ndarray, y: np.ndarray): ... @abstractmethod def predict(self, X: np.ndarray) -&gt; np.ndarray: ... class LinearModel(ABCModel): def __init__(self): self._model = LinearRegression() def train(self, X, y): self._model.fit(X, y) def predict(self, X): return self._model.predict(X) . ‚ú® Example: Dataset and CIFAR10 . Another example that fits into this category is torchvision‚Äôs Dataset class. . PyTorch has its own pipeline to do data loading. To leverage the pipeline it built, you must implement the step to fetch samples from a data set. It‚Äôs like filling up a missing piece to complete a puzzle. Once it‚Äôs filled, the fetch samples can be shuffled and batched by Dataloader class to serve a neural network model. . Dataset is the abstract class that enforces this constraint. User has to inherit from this class and implement __getindex__ method for indexing a sample from a data set: . class Dataset(Generic[T_co]): #################################### ### omit the rest of the methods ### #################################### def __getitem__(self, index) -&gt; T_co: raise NotImplementedError . Notice Dataset doesn‚Äôt make use of abc module, so its ‚Äúcontract-binding‚Äù interface __getitem__ has to add a line: raise NotImplementedError. . CIFAR10 is an example of its child class. It represents CIFAR10 dataset ‚Äî a bechmark dataset for classification task in computer vision. See how CIFAR10 implements __getitem__ to fetch an image and its associated label from the data set. . class CIFAR10(VisionDataset): #################################### ### omit the rest of the methods ### #################################### def __getitem__(self, index: int) -&gt; Tuple[Any, Any]: img, target = self.data[index], self.targets[index] # doing this so that it is consistent with all other datasets # to return a PIL Image img = Image.fromarray(img) if self.transform is not None: img = self.transform(img) if self.target_transform is not None: target = self.target_transform(target) return img, target . Note that VisionDataset stems from Dataset abstract class so any inheriting class follows the same contract. . Scenario 3 . . The third scenario suitable for inheritance is exception handling. . You can segregate a more generic error into more specific error with the help of inheritance.On one hand, a child error could handle a specific type of error. It better helps developers identify the root cause of a failure. On the other hand, it remains the flexibility to fall back to its generic ‚Äúparent error‚Äù. . ‚ú® Example: ContentTooShortError and URLError . A example I took here is referenced from the builtin urllib library. It defines a number of errors customized to URL handling. . URLError indicates a generic failure caused during accessing a URL link. Such failure could have many possible causes ‚Äî invalid URL, incomprehensible status code from the server response‚Ä¶ etc. ContentTooShortError represents one of the cause: the downloaded file from the URL is incomplete, meaning the file size smaller than expected. . To reflect such hierarchy, ContentTooShortError inherits from URLError: . class ContentTooShortError(URLError): &quot;&quot;&quot;Exception raised when downloaded size does not match content-length.&quot;&quot;&quot; def __init__(self, message, content): URLError.__init__(self, message) self.content = content . Here is a sample code to show the hierarchy. Thanks to the construction of ContentTooShortError, we can catch and handle this specific error if wanted. Otherwise, we could fallback to handle it like a more generic error, such as URLError. . try: raise ContentTooShortError(&quot;content too short&quot;, &quot;&quot;) except ContentTooShortError: print(&quot;Caught by ContentTooShortError exception&quot;) except URLError: print(&quot;Caught by URLError exception&quot;) except: print(&quot;Caught by other exception&quot;) # &gt;&gt;&quot;Caught by ContentTooShortError exception&quot; try: raise ContentTooShortError(&quot;content too short&quot;, &quot;&quot;) except URLError: print(&quot;Caught by URLError exception&quot;) except: print(&quot;Caught by other exception&quot;) # &gt;&gt;&quot;Caught by URLError exception&quot; . References . . Clean Code in Python: Develop maintainable and efficient code, 2nd Edition | Inheritance and Composition: A Python OOP Guide | When to Use Inheritance and When Not to in OOP? | Inheritance Is Evil. Stop Using It. | FastAI Documentation: Callbacks |",
            "url": "https://riven314.github.io/alexlauwh314/software/design/2022/06/12/WhenInheritance.html",
            "relUrl": "/software/design/2022/06/12/WhenInheritance.html",
            "date": " ‚Ä¢ Jun 12, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Notes on "Clean Code in Python" ‚Äî Design by Contracts (DbC)",
            "content": ". What is Design by Contract (DbC) and Why Need it? . . We write a programme for a specific purpose. The programme could be either a function or a class method. So long as we feed the ‚Äúright‚Äù inputs to it, ideally it should do exactly what we intend. No more and no less. Like pop method is meant to remove the last element from its list instance in-place, and sorted function is meant to make a copy of a list instance with its elements sorted. . But we are prone to mistakes, so is a programme. In practice, it is not surprising for a programme to do or output something out of our expectation. It could go wrong for various reasons, such as erroreous logics, negligence of corner cases, or incompliant users feeding wrong inputs to the programme. The root cause could easily become intractable when the programme are nested with complex logics. For the reason above, it is naive for us to safely assume a programme does what we intend. We should have a mechanism to safeguard this. When it goes out of its intension, the mechanism should be able to surface it promptly, so as to signal the developers for a fix and isolate the error from the subsequent programmes. Design by Contract (DbC) is invented for this purpose. . DbC is a software design practice to enforce responsibilities on both parties, programme and user, to ensure a programme to do what we intend. As the name suggests, the practice works like a contract. It entails the responsibilities that both parties have to observe. So long as the responsibilities are satisfied, it garantee the programme behaves correctly. If the programme goes wrong, it could quickly surface the issue and easily trace which party breaks its responsibilities. . Key Pillars of Design by Contract . . Just like a contract is typically composed of several sessions, DbC typically contains several components: Precondition, Postcondition, Invariants and Side Effects. It is a good practice to document them to inform the users about the essential ‚Äúterms and conditions‚Äù of the programme. . Among these components, Precondition and Postcondition are the core because they are the responsibilities applied on both parties. As an analogy, they serve like two separate inspectors in a manufacturinng factory. One responsible for gating the quality of the raw materials, another responsible for checking the quality of the end product. The factory could functions perfectly thanks to the diligence of these two inspectors. . üïç Precondition . Simply put, Precondition is the inspector gating the inputs that the user feed to the programme. The programme can‚Äôt behave as intended without the ‚Äúright‚Äù inputs. If the inspector finds that the inputs do not observe to the contract, it invalidate the inputs and raise out to the user. In this instance, the responsibility belongs to the user because one fails follow the ‚Äúterms and conditions‚Äù of the contract. . Typical things to validate on the inputs are their types and properties. For example, you could validate the input is an instance of np.ndarray with dimension of 2. It is a good habit to document these validation rules in your doc-string. For types, we could instead handily annotate them in function signature, thanks to the introduction of type annotation since Python 3.6. . Such annotation gives clear instructions to the users about the types of inputs and outputs. In addition, it is convenient to debugging because we could check the type compliance with static analysis tool such as MyPy in your IDE. This approach compacts the type information into your code, sparing additional effort to document them in doc-string. Having said that, type annotations in Python only serves as a soft compliance, meaning that failure of type compliance doesn‚Äôt raise any error. . . üõï Postcondition . Contrary to Preconditon, Postcondition is the inspector gating the outputs that the programme return. If any miscompliance is found in the outputs, it would raise out to the user. It helps isolate the issue quickly and prevent the ‚Äúwrong‚Äù outputs further propagate to subsequent programmes. . Similar to precondition, typical things to validate on the outputs are their types and properties. For example, you could validate the output has a np.ndarray type without any np.nan values. As mentioned above, we could annotate the expected types of the outputs by function signature. . üå† Invariant . Invariant is another component probably highlighted in the contract. I find this term popping up a lot in programming, but I did‚Äôt quite understand what it means at the beginning. It actually means something pretty conceptual. It refers to a property that always holds true in a programme. It‚Äôs a property that you can safely assume in order for the programme to function well. For example, sorted always returns a new list whose entries are sorted. . üö® Side Effects . Optionally, you can also highlight the side effects in the contract. . Side effects is any operations in a programme that change the state of things out of its local scope. It may sound confusing when you first come across the meaning of side effects. To make a few examples, changing the value of a global variable is a side effect because the function intends to change the state of a variable that doesn‚Äôt live in the function. Other examples of side effects are mutating the values of an input that you feed to the programme, or raising an error to terminate the entire process. . It is worthwhile to highlight the side effects, if any, because these are things that Precondition and Postcondition fail to capture. . Examples from JAX Codebase . . It‚Äôs worth taking a look at an example to consolidate the idea. As my personal preference, I take a function from JAX codebase as an example because I am recently picking up the framework - broadcast_shapes . broadcast_shapes receives any number of shape instances as inputs. A shape represents the dimensions of any tensors. It is expressed as a tuple of int. Given the shapes as inputs, broadcast_shapes attempts to find out the ultimate shape after broadcasting is applied on each axis of the inputs. . For example: . from jax.numpy import broadcast_shapes broadcast_shapes((1, 1, 5), (3, 1, 1), (1, 4, 1)) # (3, 4, 5) . üîé A Brief Look at the Source Code . broadcast_shapes internally triggers another function of same name but living in a private module jax._src.lax.lax. We extract the code snippet associated to broadcast_shapes below: . # code snippet extracted from jax._src.lax.lax def broadcast_shapes(*shapes: Tuple[Union[int, core.Tracer], ...] ) -&gt; Tuple[Union[int, core.Tracer], ...]: &quot;&quot;&quot;Returns the shape that results from NumPy broadcasting of `shapes`.&quot;&quot;&quot; # NOTE: We have both cached and uncached versions to handle Tracers in shapes. try: return _broadcast_shapes_cached(*shapes) except: return _broadcast_shapes_uncached(*shapes) @cache() def _broadcast_shapes_cached(*shapes: Tuple[int, ...]) -&gt; Tuple[int, ...]: return _broadcast_shapes_uncached(*shapes) def _broadcast_shapes_uncached(*shapes): _validate_shapes(shapes) fst, *rst = shapes if not rst: return fst # First check if we need only rank promotion (and not singleton-broadcasting). try: return _reduce(_broadcast_ranks, rst, fst) except ValueError: pass # Next try singleton-broadcasting, padding out ranks using singletons. ndim = _max(len(shape) for shape in shapes) shape_list = [(1,) * (ndim - len(shape)) + shape for shape in shapes] result_shape = _try_broadcast_shapes(shape_list) if result_shape is None: raise ValueError(&quot;Incompatible shapes for broadcasting: {}&quot; .format(tuple(shape_list))) return result_shape def _validate_shapes(shapes: Sequence[Shape]): def _check_static_shape(shape: Shape): checked = canonicalize_shape(shape) if not all(idx &gt;= 0 for idx in checked): msg = f&quot;Only non-negative indices are allowed when broadcasting&quot; f&quot; static shapes, but got shape {shape!r}.&quot; raise TypeError(msg) . If we try to apply DbC framework to analyse broadcast_shapes, we will notice it has both Precondition and Postconditions. Let‚Äôs find them out one by one! . üìç Identifying Precondition . Notice most of the heavy lifting jobs are done in _broadcast_shapes_uncached. From there we see that _validate_shapes is the helper function that validate the inputs so it is in fact serving as Precondition. . def _validate_shapes(shapes: Sequence[Shape]): def _check_static_shape(shape: Shape): checked = canonicalize_shape(shape) if not all(idx &gt;= 0 for idx in checked): msg = f&quot;Only non-negative indices are allowed when broadcasting&quot; f&quot; static shapes, but got shape {shape!r}.&quot; raise TypeError(msg) . What _validate_shapes does is to standardize (they call it canonicalize) the types of each shape instance and verify that they all only contain positive int. Otherwise, the programme raises a TypeError. If we find this error upon using broadcast_shapes, it means we are complying to the rules set in the contract. . Additionally, we can see from broadcast_shapes‚Äôs signature that each shape is expected to have a type of Tuple[int, ...]. . . Note: As a side note, you will notice the type annotation of shape is actually Tuple[Union[int, core.Tracer], ...]. core.Tracer is a special type designed by JAX specifically used for abstract evaluation rules. This type is only internally used in JAX and its function is out of the scope of this blogpost. You could safely ignore it for now. üìå Identifying Postcondition . From broadcast_shapes‚Äôs signature, we can see that a new shape instance is expected to return and it should have a type of Tuple[int, ...]. . _try_broadcast_shapes attempts to find a valid shape as a return but it doesn‚Äôt garantee to find one because the shapes provided may not be compatible to the broadcasting rules. Therefore, the programme has to check if a valid shape has been found at the end. The way it does is to check if result_shape is None. The programme raises ValueError if no valid shape is found. . if result_shape is None: raise ValueError(&quot;Incompatible shapes for broadcasting: {}&quot; .format(tuple(shape_list))) . However, there is one caveat here. When it comes to which party is responsible, the context here is slightly different. Failure of Postcondition doesn‚Äôt imply a critical flaw in broadcast_shapes. Instead it means by nature broadcasting can‚Äôt be done on the provided shapes. broadcast_shapes implicitly assumes its users understand the broadcasting rules. . References . . Clean Code in Python: Develop maintainable and efficient code, 2nd Edition | JAX Github Codebase | Wikipedia: Design by contract |",
            "url": "https://riven314.github.io/alexlauwh314/software/design/2022/06/03/DesignByContract.html",
            "relUrl": "/software/design/2022/06/03/DesignByContract.html",
            "date": " ‚Ä¢ Jun 3, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Quick Guide for Installing NVIDIA Driver, CUDA Toolkit and cuDNN",
            "content": "Motivations . . If you work in machine learning or deep learning, you probably have experience installing NVIDIA driver, CUDA Toolkit and cuDNN. They are essential softwares to enable GPU computing for most Deep Learning frameworks. But installing them correctly can be tricky. Any step goes wrong could leave you clueless of what to do. . This post offers you a quick guide to installing NVIDIA driver, CUDA Toolkit and cuDNN, coupled with things you should pay attention to. This guide consolidates the resources I used when I installed the same for my GPU server. . . Note: The guide is specific to Ubuntu and the version I am using is Ubuntu 20.04 LTS. The steps may or may not work for other Ubuntu version. Step 1: Install NVIDIA Driver . . Option 1: Install via ‚ÄúSoftware &amp; Updates‚Äù . From ‚ÄúSoftware &amp; Updates‚Äù, go to ‚ÄúAdditional Drivers‚Äù for available GPU drivers you can install | Choose the driver you need (those labelled as ‚Äúproprietary, tested‚Äù is more flavorable) | Click ‚ÄúApply Changes‚Äù and wait for the installation to complete | Reboot your machine | . Option 2: Install via Command Line . Search for available NVIDIA drivers in your terminal apt search nvidia-driver . | Update your package repository sudo apt update sudo apt upgrade . | Install the NVIDIA driver that you need (in my case, I installed nvidia-driver-470) sudo apt install nvidia-driver-470 . | Reboot your machine | If successful, you should be able to run the command nvidia-smi after reboot. The following is a sample output: . alex@alex-desktop:~$ nvidia-smi Thu Mar 31 17:12:04 2022 +--+ | NVIDIA-SMI 470.103.01 Driver Version: 470.103.01 CUDA Version: 11.4 | |-+-+-+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA GeForce ... Off | 00000000:01:00.0 On | N/A | | 30% 35C P8 21W / 250W | 276MiB / 11019MiB | 2% Default | | | | N/A | +-+-+-+ | 1 NVIDIA GeForce ... Off | 00000000:02:00.0 Off | N/A | | 30% 33C P8 2W / 250W | 10MiB / 11019MiB | 0% Default | | | | N/A | +-+-+-+ +--+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 N/A N/A 1019 G /usr/lib/xorg/Xorg 71MiB | | 0 N/A N/A 1612 G /usr/lib/xorg/Xorg 91MiB | | 0 N/A N/A 1739 G /usr/bin/gnome-shell 100MiB | | 1 N/A N/A 1019 G /usr/lib/xorg/Xorg 4MiB | | 1 N/A N/A 1612 G /usr/lib/xorg/Xorg 4MiB | +--+ . Step 2: Install CUDA Toolkit . . Download the .deb file of your desired CUDA Toolkit version Here is a menu of all CUDA Toolkit archive(you need to register a NVIDIA membership to view the page) | This chart summarizes what version of CUDA Toolkit is compatible with your NVIDIA driver(extracted from CUDA Toolkit documentation) | . . | Run the following commands in your terminal to install CUDA Toolkit. Make sure the current directory contains the downloaded .deb file (in my case I installed CUDA Toolkit 11.4) dpkg -i cuda-repo-ubuntu2004-11-4-local_11.4.0-470.42.01-1_amd64.deb sudo apt-key add /var/cuda-repo-ubuntu2004-11-4-local/7fa2af80.pub sudo apt-get update sudo apt-get install cuda . | Append the following environment variables in your ~/.bashrc file export PATH=${PATH}:/usr/local/cuda-11.4/bin export CUDA_HOME=${CUDA_HOME}:/usr/local/cuda:/usr/local/cuda-11.4 export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/usr/local/cuda-11.4/lib64 . | Run source ~/.bashrc in your terminal to update the environment variables of your bash session | If successful, you should be able to run the command nvcc --version. Here is a sample output: . alex@alex-desktop:~$ nvcc --version nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2021 NVIDIA Corporation Built on Wed_Jun__2_19:15:15_PDT_2021 Cuda compilation tools, release 11.4, V11.4.48 Build cuda_11.4.r11.4/compiler.30033411_0 . . Warning: There are a few things to pay attention!First, I dont suggest to install CUDA Toolkit via package repository (i.e. sudo apt-get install cuda) because it would install outdated version. I got CUDA Toolkit version 10.x when installed via package repository.Second, I suggest to delete the old .deb files from your current directory before running sudo apt-get intsall cuda because it would pick up the latest version of .deb file in the folder for installation.Finally, double check the CUDA Toolkit version to be installed is compatible with your NVIDIA driver based on the .deb filename. For example, mine is cuda-repo-ubuntu2004-11-4-local_11.4.0-470.42.01-1_amd64.deb, so it is compatible with NVIDIA driver 470. Step 3: Install cuDNN . . Download the .tgz file of your desired cuDNN version Here is a menu of all cuDNN archives. Pick the one compatible with your CUDA Toolkit version (e.g. Mine is cuDNN v8.2.4 for CUDA 11.4) | . | Run the following commands in your terminal to install cuDNN. Make sure the current directory contains the downloaded .tgz file (In my case, I installed cuDNN version 8.2.4) tar -xzvf cudnn-11.4-linux-x64-v8.2.4.15.tgz sudo cp cuda/include/cudnn.h /usr/local/cuda/include sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64 sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn* . | . Warning: check your .tgz filename to make sure it is compatible with your CUDA Toolkit version. For example, mine is cudnn-11.4-linux-x64-v8.2.4.15.tgz, so it‚Äôs compatible with CUDA Toolkit 11.4 How Do I Know Everything is Working? . . A straight-forward way to tell is to install CUDA version of any Deep Learning framework (e.g. PyTorch, Tensorflow, JAX) and see if they managed to detect your GPU devices. . The following Python code help you validate CUDA is working in each framework: . # PyTorch import torch torch.cuda.is_available() # Tensorflow import tensorflow as tf tf.config.list_physical_devices(&#39;GPU&#39;) # JAX import jax jax.devices() . What If I Failed the Installation? . . it is not uncommon to encounter something unexpected during your installation (e.g. you got an error during CUDA installation, or command nvcc --version doesn‚Äôt work after installing CUDA Toolkit). . Fear Not! What you need to do is to remove all of your installations (i.e. NVIDIA Driver, CUDA Toolkit, cuDNN) and try the installation from scratch again. . To remove all of your installations, run the following commands in your terminal: . apt clean apt update apt purge cuda apt purge nvidia-* apt autoremove . References . . Install Nvidia Drivers on Ubuntu 20.04 {3 Methods} | Best practice for upgrading CUDA and cuDNN for tensorflow | Problem while installing cuda toolkit in ubuntu 18.04 | Verify CUDA Installation - xCAT 2.16.3 documentation | CUDA Toolkit Archive | cuDNN Archive | Release Notes :: CUDA Toolkit Documentation |",
            "url": "https://riven314.github.io/alexlauwh314/cuda/devops/2022/03/31/Quick-Guide-Install-CUDA.html",
            "relUrl": "/cuda/devops/2022/03/31/Quick-Guide-Install-CUDA.html",
            "date": " ‚Ä¢ Mar 31, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Paper Summary: "MVSNet: Depth Inference for Unstructured Multi-view Stereo"",
            "content": ". 1. Motivations . . Multi-view stereo is a general technique of 3D scene reconstruction by associating correspondence between multiple views. Views from the same scene typically share high degree of content. We can leverage the point-wise associations between views to recover the 3D points of the shared content. . There are different approaches to find the correspondences. One classical approach is plane sweeping stereo. It esimates per-view depth map, and then jointly utilised the depth maps to estimate the 3D point clouds. This approach is scalable because it can process each view independently with low memory footprint. Despite its scalability, it struggles to recover challenging scenarios, such as region with occlusion, or surface that are specular or low-textured. It is hard to find correspondences of these regions using hand-crafted features. This issue could lead to noisy estimation of depth map. . To address the issue, the authors proposes MVSNet: an end-to-end deep learning pipeline resembling traditional sweeping plane approach. It is as scalable as traditional sweeping plane approach, with more robust depth map estimation. . . 2. Intuition of Plane Sweeping Stereo . . Before diving into MVSNet‚Äôs pipeline, let‚Äôs have a quick look of how plane sweeping stereo works. Understanding its key idea helps understand the basis that MVSNet is built upon. . The objective is to estimate a depth map for a view. Let‚Äôs call the view as reference view $ bold{I}_{1}$ and denote the other views as source views $ { bold{I}_{k} }_{k=2}^N$. The algorithm makes use of all $N$ views (both reference view and source views) to do the estimation. . Imagine you insert a suite of virtual planes into the scene that are parallel to the reference view. Let‚Äôs say there are $D$ of them. These planes are uniformly apart with varying distance from the reference view. These planes represent different hypotheses of depth values. . Next, we cast a viewing frustum of the reference camera into the scene. Following the frustum we project the reference view into each virtual plane. We end up with $D$ projections, essentially different enlargment of the reference view. . For the rest of the views, we can warp each of them into each virtual plane, at the same place where the reference view is projected. Since we know camera matrix of each view, we can apply classical homography to warp a source view to the target projection, and then express its image coordinate with respect to reference camera. . After warping all views, we should have $N$ warped images on each plane. We can then evaluate the pixel-wise variance across the $N$ warped images. The intuition is to estimate how likely the $N$ warped images are projecting the same content at the same pixel position. Low variance indicates the 3D point associated to the target pixel is likely to lie on that plane. Therefore, we can estimate the depth of the target pixel by the depth of that plane. . The above procedures can be visualized by the following diagram. For simplicity, we use colour as a naive feature. Notice the target pixel has the lowest variance in colour at the second plane, so its associated depth $d_{2}$ is the best estimate of target pixel‚Äôs depth. . . . Note: Since this session aims to offer high-level intuition, I skipped its technical details. Also for the diagram above, I use colour as feature but it is usually not favorable because it is vulnerable to variations like illuminance difference. 3. Feature Extraction and Feature Map Warping . . 3.1. 2D CNN $f_{ bold{F}}$ for Feature Extraction . MVSNet‚Äôs pipeline is basically similar to plan sweeping stereo, except it swaps in learnable components into a few places. . Knowing that hand-crafted feature is not robust to diverse variations, MVSNet deploys a learnable 2D Convolutional Neural Network (CNN) $f_{ bold{F}}$ to extract features from each view. The extracted features should capture rich local semantics, so it is more robust to different variations. . fF(Ii)=Fif_{ bold{F}}( bold{I_{i}}) = bold{F_{i}}fF‚Äã(Ii‚Äã)=Fi‚Äã . Let‚Äôs say a view $ bold{I_{i}}$ has a dimension of $(W times H times 3)$ including RGB channel. The CNN returns its downsampled feature map $ bold{F_{i}}$ of dimension $(W/4 times H/4 times F)$, with $F$ being the size of feature channel. $f_{ bold{F}}$ serves as a feature extractor. It transforms a set of views $ { bold{I}_{k} }_{i=1}^{N}$ into a set of feature maps $ { bold{F}_{k} }_{i=1}^{N}$, visualized as follows. . . 3.2. Differential Homography Applied on Feature Maps . By the principle of homography, we can warp feature maps to each sweeping plane in front of the reference view. . Since the homography transformation is just a matrix multiplication with a feature map, it is differentiable and can be backpropagated. Such nice property makes homography transformation integrable into the end-to-end pipeline. This enables MVSNet to be aware of geometric relationships between different views, so as to learn the 3D geometry of the scene. . A set of feature maps $ { bold{F}_{k} }_{i=1}^{N}$ of dimension $(W/4 times H/4 times F)$ are warped to a set of feature volumes $ { bold{V}_{k} }_{i=1}^{N}$ of dimension $(W/4 times H/4 times D times F)$. . . 4. Cost Volume and Probability Volume . . 4.1. Constructing Cost Volume $ bold{C}$ . Now we need to aggregate the set of feature volumes $ { bold{V}_{k} }_{i=1}^{N}$ into a single tensor. This is tricky because the number of views $N$ is arbitrary. . The authors propose variance-based metric to reduce the feature volumes set into a tensor of dimension $(W/4 times H/4 times D times F)$. The metric is essentially an element-wise variance across feature volumes. The variance-based metric is in line with the principle of traditional plane sweeping stereo. It measures multi=feature consistency at different pixel position at different depth. . The tensor it returns is called Cost Volume $ bold{C}$. . C=Var(V1,...,VN)=‚àëi=1N(Vi‚àíVÀâ)2N bold{C} = text{Var}( bold{V_{1}},..., bold{V_{N}}) = frac{ sum_{i=1}^{N} ( bold{V_{i}} - bold{ bar{V})^2}} {N}C=Var(V1‚Äã,‚Ä¶,VN‚Äã)=N‚àëi=1N‚Äã(Vi‚Äã‚àíVÀâ)2‚Äã . Here $ bold{ bar{V}}$ is the element-wise average of $ { bold{V}_{k} }_{i=1}^{N}$ and the whole operation is differentiable. . 4.2. Constructing Probability Volume $ bold{P}$ . Notice the Cost Volume encodes multi-feature consistency in its $F$ channel. It is hard to infer depth based on those high-dimensional quantities. . In addition, the Cost Volume is especially noisy on regions with occlusion or tricky surface. To make the subsequent depth estimate more accurate, we can apply smoothing on the Cost Volume. It could effectively smooth away noise based on information from its neighboring pixels. . To this end, the authors introduce a learnable 3D CNN UNet $f_{ bold{P}}$ to transform the Cost Volume into a tensor of dimension $(W/4 times H/4 times D)$. It serves to aggregates $D$ channel and smooth away the noise from the Cost Volume. . fP(C)=Pf_{ bold{P}}( bold{C}) = bold{P}fP‚Äã(C)=P . The last layer is a softmax applied on $D$ channel, so the resultant tensor is a Probability Volume $ bold{P}$ which expresses the pixel-wise depth as a probability distribution. You can treat the pixel-wise depth estimation as a classification problem. The probability distribution signals which sweeping plane the associated 3D point is likely to lie on. . . 5. Loss Function . . 5.1. Initial Depth Map $ bold{D}_{i}$ . Argmax (winner-take-all) is not differentiable so we shouldn‚Äôt use it to get the estimated depth map from Probability Volume. . An alternative is Soft Argmin operation. It is no different from a pixel-wise expectation over the probability distribution of its depth. We can estimate the initial depth map $ bold{D_{i}}$ with soft argmin operation. Its dimension is $(W/4 times H/4)$. . The predicted depth at pixel position $(u, v)$ is: . Di(u,v)=‚àëd=dmindmaxd√óP(u,v,d) bold{D}_{i}(u, v) = sum_{d=d_{min}}^{d_{max}} d times bold{P}(u, v, d)Di‚Äã(u,v)=d=dmin‚Äã‚àëdmax‚Äã‚Äãd√óP(u,v,d) . $d_{min}$ and $d_{max}$ is the depth of the closest and furthest sweeping plane, while $ bold{P}(u, v, d)$ is the probability that pixel position $(u, v)$ has a depth of $d$. . 5.2. Refined Depth Map $ bold{D}_{r}$ . Initial depth map may suffer from over-smoothing on its boundary caused by the 3D CNN UNet. . To address this, the authors introduce a 2D CNN $f_{r}$ to predict a residual map from the initial depth map $ bold{D_{i}}$ and the resized reference image $ bold{ tilde{I}}_{1}$. $ bold{D}_{i}$ and $ bold{ tilde{I}}_{1}$ are concatenated before feeding to network. . Composing the residual and the initial depth map gives a refined depth map $ bold{D}_{r}$: . fr(Di,I~1)+Di=Drf_{r}( bold{D}_{i}, bold{ tilde{I}}_{1}) + bold{D}_{i} = bold{D}_{r}fr‚Äã(Di‚Äã,I~1‚Äã)+Di‚Äã=Dr‚Äã . 5.3. Putting Them Together as Supervision . To complete the training pipeline, we compare the ground truth depth map against both the initial depth map and the refined depth map. . All 3 networks $f_{ bold{F}}$, $f_{ bold{P}}$, $f_{r}$ are learned to minimise the absolute differences. . . 6. Results . . Once MVSNet is trained, you can deploy it to estimate the depth map for a view. . The inference pipeline works the same as training one. You treat the view of interest as a reference view, and then warp all views onto its associated sweeping planes. You feed them to the trained MVSNet, and it runs through the pipeline to give you its estimated depth map. What makes it even more powerful is its capacity to generalize on unseen scenes. Even if the input views are from novel scenes, MVSNet is still able to reliably estimate their depth maps. . The authors propose an additional post-processing step to further filter the depth map. . . After post-processing, MVSNet significantly outperform preceding models. The authors compare its reconstruction quality against different models for various scenes. It shows that preceding models fail to reconstruct a number of regions. In contrast, MVSNet managed to recover those regions. . The improvement can be attributed to the robust feature representation learned by $f_{ bold{F}}$. Additionally, $f_{ bold{P}}$ and $f_{r}$ (together with post-processing) further improve the quality of the predicted depth map. . . Quantitatively, MVSNet outperforms preceding models by a large margin in most aspects, as shown: . . 7. References . . MVSNet: Depth Inference for Unstructured Multi-view Stereo | A space-sweep approach to true multi-image matching | Multi-View Stereo: A Tutorial | Fall 2021 CS 543/ECE 549: Computer Vision, Lecture 18 Slide (University of Illinois at Urbana-Champaign) | Homography for tensorflow | Pinhole Camera: Homography (written in Chinese) |",
            "url": "https://riven314.github.io/alexlauwh314/paper/computer-graphics/2022/03/29/MVSNet.html",
            "relUrl": "/paper/computer-graphics/2022/03/29/MVSNet.html",
            "date": " ‚Ä¢ Mar 29, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Paper Summary: "BlockNeRF: Scalable Large Scene Neural View Synthesis"",
            "content": ". 1. Motivations . . BlockNeRF has recently taken the Twitter‚Äôs ML community by storm. Intrigued by its amazing demo of realistic driving views at different landmarks, I am hooked to read its paper. . Block-NeRF: Scalable Large Scene Neural View Synthesisabs: https://t.co/xDa1bmJaCJproject page: https://t.co/IQTnknxTuK pic.twitter.com/HX7cSx6Yh1 . &mdash; AK (@_akhaliq) February 11, 2022 Jointly developed by Google and Waymo, BlockNeRF extends NeRF to reconstruct realistic city map at large scale. This is not the first work on scalable scenes with NeRF. To name a few, MegaNeRF reconstructs wild landscape with drone data, and CityNeRF reconstructs map in city scale. However, BlockNeRF is the first work applying NeRF on automonous driving. . How does BlockNeRF help with automonous driving? Automonous vehicles requires sheer volume of driving scenarios for training but collecting them first hand at scale is expensive. As an alternative, researchers actively explore the possibility to simulate realistic driving scenes with deep learning models. One proposal is to simulate scenes from a virtual world (just imagine the scenes you see in GTA). However, it is undesirable because the simulated scenes are neither representative to reality nor diverse. On the contrary, BlockNeRF is capable to simulate realistic driving scenes with huge variety. . 2. An Overview of BlockNeRF‚Äôs Architecture . . BlockNeRF is a variant of Neural Radiance Field network (aka NeRF). As a very brief introduction, NeRF learns to reconstruct a 3D scene from its 2D views and their associated viewpoints. We can treat a scene as a bounded volume and NeRF describes the density $ sigma$ and RGB colour $ bold{c}$ of any position in the space. Once trained, NeRF can project the 3D scene into 2D image at any viewpoints. It deploys a classical technique called volumetric rendering to render each pixel by tracing a ray from the viewpoint, through the pixel, to the scene. . I will highlight some key techniques by BlockNeRF, together with its architecture. Due to limited space, I will spare their nitty gritty details. . Below diagram best summarises BlockNeRF‚Äôs design and I will explain each component in the following sessions. . . 2.1. Segregating Sub-networks $f_{ sigma}$ and $f_{ bold{c}}$ . BlockNeRF divides a gigantic scene into blocks and then conquer each block with an individual NeRF network. But unlike standard NeRF, it segregates the prediction of density and colour into 2 different sub-networks, namely $f_{ sigma}$ and $f_{ bold{c}}$. . 2.1.1. Density Prediction by $f_{ sigma}$ . The 1st sub-network $f_{ sigma}$ predicts the density $ sigma$ at position $ bold{x}$. The sub-network only requires position as input because geometry only depends on space. No matter how you view the same position at different angles or at varying lighting conditions, its geometry should be the same. Besides $ sigma$, the sub-network $f_{ sigma}$ spits out a high-dimensional embedding, to be consumed by the 2nd sub-network $f_{ bold{c}}$. . One technique worth highlighting is Integrated Positional Encoding (borrowed from Mip-NeRF). When rendering a pixel‚Äôs colour, standard NeRF treats a position as a point sampled from a beam of traced ray. Such setting makes NeRF unaware of its distance from the viewpoint because the same point yields the same feature no matter where you look at it. The consequence is a degraded rendering quality when NeRF has to render views in a wide range of scales (e.g. extreme close/ distant shot of the scene). . Instead of treating position as a point from a beam of ray, we can treat it as a conical frustum from a cone of rays. As a result, the network can reason about the distance of a position from the viewpoint based on the shape and size of the conical frustum. The same position traced from a further viewpoint should yield a larger conical frustum, illustrated below: . . Such reframing implies that we have to treat $ bold{x}$ as a sample randomly drawn from a distribution resembling the conical frustum. Simulate $ bold{x}$ probabilistically would make the computation intractable, so we can featurize $ bold{x}$ by its expectation instead. To further simplify the setting, we can approximate ithe ill-shaped volumetric distribution by a nice-shaped multivariate Gaussian distribution. Such approximatation gives us a closed form on expected $ bold{x}$ transformed by positional encoding, aka $ mathbb{E}( gamma( bold{x}))$. . . . Note: Since this post aims to intuitively explain the concept so I will omit the closed-form formula in this post. As a remark, this technique is called Integrated Positional Encoding because an expectation is no different from an integration over a probability distribution. 2.1.2. Colour Prediction by $f_{ bold{c}}$ . The 2nd sub-network $f_{ bold{c}}$ predicts the RGB colour. Unlike density, colour requires more than position to describe. Here I list out factors that $f_{ bold{c}}$ considers: . Factor 1: Spatial Position It is trivial to say that a scene has different colours at different positions. We make use of the embedding emitted from $f_{ sigma}$ to featurise this spatial information. As a recap of $f_{ sigma}$, the embedding featurises the position as a volume instead of a point, so $f_{ bold{c}}$ shares the same advantage as $f_{ sigma}$ to be aware of its distance from the viewer. . Factor 2: Viewing Direction Depending on light source, an object‚Äôs surface has different brightness at different angles, hence difference in colours. We can tell a surface‚Äôs brightness from its illuminance - a technical term to measure the amount of lights spreading over a surface area. We can use unit vector $ bold{d}$ to capture the direction where we look at a point. . Factor 3: Exposure Level of a Camera Exposure describes the amount of lights exposed to a camera‚Äôs film. High exposure leads to a bright view while low exposure leads to a dim view. Below diagram contrasts the same view at different exposure levels: . . Exposure level is mainly controlled by shutter speed - a measurement of time when camera‚Äôs shutter opens for light to come in. High shutter speed captures sparse light and results in sharp image, while low shutter speed captures dense lights and results in blurry motion. Training images are collected in the wild so it is subject to a wide range of exposure levels. Conditioning $f_{ bold{c}}$ on exposure level helps it account for the variation. . BlockNeRF parameterises exposure level by shutter speed and 2 additional scaling factors: analog gain and $t$. It is further transformed by positional encoding with 4 levels, i.e. $ gamma_{4}(.)$. . Exposure=Œ≥4(shutter¬†speed√óanalog¬†gain/t) begin{split} . text{Exposure} = gamma_{4}( text{shutter speed} times text{analog gain}/t) . end{split}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;Exposure=Œ≥4‚Äã(shutter¬†speed√óanalog¬†gain/t)‚Äã&lt;/span&gt;&lt;/span&gt; . Factor 4: Environmetal and Photometric Variations Besides exposure level, training images are also subject to environmental variations (e.g. change in day of time/ weather) and photometric variations (e.g. different photographic visual effects). These variations could break standard NeRF because we could have training images of different appearances even at the same viewpoint (e.g. imagine day view v.s. night view). Such inconsistency confuses standard NeRF because it assumes the scene is static and controlled so same viewpoint should lead to same appearance . . Therefore, the authors apply a technique called Appearance Embedding (borrowed from NeRF in the Wild). They introduce an auxiliary latent vector as an input to the network in order to describe the variations. Each image is assigned a latent vector and it helps explain away the variation of appearance specific to the image. Once learned, we can render novel views with any of these latent vectors to control appearance. Want to render a novel view with a night view like image A? Just append its associated latent vector when rendering! . The latent vectors are optimised alongside with the network through a technique called Generative Latent Optimization. You can treat it as an additional optimisation independently applied on each latent vector, with an objective to minimise the reconstruction loss against its associated image. . 2.2. Visibility Prediction by $f_{v}$ . Besides blocks of NeRF, we have a visibility network $f_v$ to holistically scan through the scene. It is inefficient to query all blocks when rendering, so visibility network helps filter blocks that gives material contributions. It evaluates how visible a place is from a viewpoint. If most places of the block are not visible from the viewpoint, then we can safely discard the block for rendering. . We could quantify the visibility of a place by its transmittance. It tells the chance of a ray passing from a viewpoint to the place. The densities along the way influence the transmittance because the ray may get blocked during its travel. Transmittance of close to 1 means high transparency along the way, close to 0 means it is likely blocked by solid particles along the way. . How to compute transmittance with the visibility network? Let‚Äôs say we are at position $ bold{o}$ viewing at direction $ bold{d}$, we can march a ray from our viewpoint and parameterise it as $ bold{r}(t) = bold{o} + t bold{d}$, where $t$ describes the distance from the viewpoint. Then we can collect discrete samples along distances $ { t_k }_{k=1}^{N}$ and evaluate their respective densities $ { sigma_k }_{k=1}^{N}$ with the visibility network. With the quantities, we can numerically approximate the transmittance at distance $t_{i}$ by: . Ti=exp‚Å°(‚àí‚àëj&lt;iŒ¥jœÉj),Œ¥l=tl‚àítl‚àí1 begin{split} . T_{i} = exp(- sum_{j&lt;i} delta_{j} sigma_{j}), hspace{8 mm} delta_{l} = t_{l} - t_{l-1} . end{split}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;Ti‚Äã=exp(‚àíj&lt;i‚àë‚ÄãŒ¥j‚ÄãœÉj‚Äã),Œ¥l‚Äã=tl‚Äã‚àítl‚àí1‚Äã‚Äã&lt;/span&gt;&lt;/span&gt; . Compared against $f_{ sigma}$ and $f_{ bold{c}}$, visibility network $f_{v}$ is light-weight to efficiently query across blocks. It deploys the same Integrated Positional Encoding trick to capture positional information. . 2.3. Masking Out Transient Objects . BlockNeRF targets to model the static landscape of the scene, but transient objects (e.g. pedestrians/ vehicles) in the training images obstruct its learning because their presence leads to view inconsistency. Below are examples where the same landmark have different groups of pedestrians at different time. . . To address transient objects, we could bound their belonging regions with a pretrained semantic segmentation model, and then exclude the regions from the supervision. Recall that NeRF is supervised by minimising reconstruction loss between ground-truth pixels and rendered ones. Removing pixels associated to transient objects help eliminate inconsistent signals to NeRF. . . . Note: This trick is also borrowed from NeRF in the Wild! 3. Orchestrating the Blocks . . 3.1. Composing Blocks for Rendering . Once trained, we can render any novel view with the blocks. The simplest way to render a view during inference is to look at the block nearest to the viewpoint. However, the paper shows such naive approach leads to temporal inconsistency when animating a driving views across blocks. It creates abrupt jump in appearance upon block transition. . To make the views more consistent, we can blend the views rendered by several blocks. Doing so is computationally expensive so we can apply simple heuristics to filter relevant blocks. One simple rule is to scope the blocks within a fixed radius of the viewpoint. On top of that, we utilise visibility network $f_{v}$ to assess the overall contributions of each scoped block. We remove the block from final rendering if its average visibility (measured by transmittance) is below a certain threshold. . After these 2 simple filtering, 1-3 blocks remain for final blending. We can blend them together by interpolation. We use inverse distance weighting to determine the weight of each view: . wi=distance(c,xi)‚àíp begin{split} . w_{i} = text{distance}(c, x_{i})^{-p} . end{split}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;wi‚Äã=distance(c,xi‚Äã)‚àíp‚Äã&lt;/span&gt;&lt;/span&gt; . $c$ is the viewpoint‚Äôs coordinate and $x_{i}$ is the coordinate of the block‚Äôs center. $p$ is a paramter that controls the rate of blending. The authors experimented different distance measures and interpolation methods. They conclude that 2D Inverse Distance Weighting (IDW 2D) yields that best results in terms of temporal consistency. . . 3.2. Aligning Appearance Across Blocks . We know we have to blend views from different blocks for rendering, but a key detail is missing: How do the views align their appearance? This is tricky because the blocks don‚Äôt share any agreement on how to decode the latent vector into appearance. Different blocks can decode the same latent vector as different appearances. . To address this, we performs an additional optimisation to enforce the consistency of appearance across blocks. Let‚Äôs say we want to render a views with a target appearance seen in a training image, we can retrieve its associated latent vector (target latent vector) from one of the responsible blocks (target block). With the target block and target latent vector, we can do the following: . Freeze the target latent vector and network weights of all blocks | Select a block adjacent to the target block | Retrieve a latent vector for the adjacent block | Render a view at a place commonly seen by both target block and the adjacent block | Optimise the latent vector to close the difference between the view rendered by target block v.s. the adjacent block | Note that the paper doesn‚Äôt mention how the adject block retrieves a latent vector for optimisation. My educated guess is that it works either by randomly sampling one from its learned latent vectors or initialising from scratch. Regardless of how it is retrieved, it will eventually converge towards the target appearance. And the optimisation should be fast because we only need to optimise a latent vector. . At the beginning of the optimisation, we could expect the same view rendered by the 2 blocks have different appearances. But over the optimisation, the view rendered by the adjacent block will converge towards the target appearance, illustrated below: . . After that, we can propagate the same scheme of optimisation outward to other blocks. At the end all blocks have a latent vector responsible for the target appearance. Despite a perfect alignment is hard to achieve, this solution should preserve consistency on most macro attributes of the scene. . To make the above proposal work, each block has to overlap with its neighbours. The authors configure the blocks so that any 2 adjacent blocks have at least 50% overlap. . 4. Results . . To showcase the power of BlockNeRF, the authors train BlockNeRF on 2.8 million posed images captured in San Francisco. They are collected on public roads with data collection vehicles from June 2021 to August 2021. . Since the training datasets are driving views collected by vehicles, so this work focuses on rendering novel driving views. The picture below cherry-picks the rendered views of different driving locations at Alamo Square (San Francisco). The pair of views on the right shows the modularity of BlockNeRF. There was a construction work undergoing on June 2021, which was completed at September 2021. If we want to update the scene with the renovated building, we only need to retrain the responsible blocks without the need to train all blocks again. . . Here BlockNeRF animates a driving view at Moscone Center (San Francisco). You can see the night view smoothly transits to the day view thanks to the interpolation of their respective latent vectors. . . Below is the same trick. BlockNeRF transits Downtown (San Francisco) from dawn to morning by interpolation of the learned appearance embeddings. . . BlockNeRF can offer diverse driving scenarios because it is easily extensible and you can control both the exposure level and the appearance of a scene from model inputs. . 5. Limitations . . The authors list out some limitations of their work at the end. Here I highlight a few: . Heavy computational footprint due to large scale training, hence the carbon emissions | Inability to model dynamic scenes, solving which could unlease NeRF application on robotics | Background is rendered blurry because limited blocks are used after filtering and they don‚Äôt cover most background | Slow rendering speed and long training time, issues that other NeRF variants commonly suffer | . 6. References . . CSC2547 NeRF in the Wild Neural Radiance Fields for Unconstrained Photo Collections | Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields | Block-NeRF: Scalable Large Scene Neural View Synthesis | NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections | Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields | INeRF: Inverting Neural Radiance Fields for Pose Estimation | Optimizing the Latent Space of Generative Networks | .",
            "url": "https://riven314.github.io/alexlauwh314/paper/computer-graphics/2022/03/06/BlockNeRF.html",
            "relUrl": "/paper/computer-graphics/2022/03/06/BlockNeRF.html",
            "date": " ‚Ä¢ Mar 6, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Survival Guide for Docker & Docker Compose",
            "content": ". 1. Who Should Read This? . . You probably know what Docker is and how they are commonly used in practice, but are looking for a quick guide to navigate through the jungle. This article summarises the basics in Docker and Docker Compose to help you get started. . This is originally a note for Data Engineering Zoomcamp lesson 1, and over time I expanded it with additional references. Now I decided to package it as a stand-alone survival guide with a hope to help those in my situation, who intend to quickly get themselves up for this amazing tools. . 2. Essential Docker Commands . . docker run &lt;image-name&gt;:&lt;tag&gt; &lt;command&gt; . The command does the following in order: . look up target image &lt;image-name&gt; of specified tag &lt;tag&gt; locally, otherwise download it from Docker Hub (if &lt;tag&gt; is not specified, it assumes the tag to be latest) | spin off a container from the downloaded image | execute &lt;command&gt; in the running container, if provided | gracefully stop the container after the &lt;command&gt; finishes | Remarks: . each execution of the command spins off independent container | absolute path should be used if a path is needed in docker run | . . Highlighted Flags . Flag Description . --detach (or -d) | run the container in background | . --rm | delete the container at the end of the command. useful if you intend to run the container once only | . --name | specify a name to your container. useful for inter-container communication because other containers could address your container by its name instead of its IP address. | . --env (or -e) | set an environment variable in the container. useful when the service to be containerized requires environment variables, repeatedly add --env for multiple environment variables. | . -it | attach to the session in the container at the end and enable you to interact with the session, useful when you have appended command and it | . --label (or -l) | add a meta label to the container. it‚Äôs useful if you want to group containers by labels, same as --env you can repeatedly add --label for multiple meta labels | . --volume (or -v) | mount a directory in localhost to a directory in the container e.g. --volume &lt;local-directory&gt;:&lt;container-directory&gt; | . --expose (or -p) | binds a port in local host to a port in container. useful for localhost to communicate with the containerized service e.g. --expose &lt;local-port&gt;:&lt;container-port&gt; | . --entrypoint | run specified commands in the container, functionally similar to &lt;command&gt; in this case. | . Examples . docker run python:3.8 ls: spin off a container from image python:3.8 and then list the folders &amp; files at default directory of the container | docker run --rm -d ython:3.8 python -m http.server: start a http server at default directory of a container and run the container in background (running in background is useful here, otherwise your terminal session can‚Äôt resume unless you forcefully shut down the server). upon shutting down the container, it is automatically deleted (thanks to --rm). | docker run -it ubuntu bash: spin off a container from ubuntu:latest image. execute bash in the container and then attach to the associated bash session (thanks to -it). | docker run -it -e POSTGRES_USER=&quot;root&quot; -e POSTGRES_PASSWORD=&quot;root&quot; -e POSTGRES_DB=&quot;ny_taxi&quot; -p 5432:5432 postgres:13: spin off a container from postgres:13 image and set 3 environment variables for the postgres database in the container. additionally the container exposes its port 5432 to local host‚Äôs port 5432. then the container starts postgres server and we attach to the associated session. | . docker container ls . This command lists out attributes for each container, by default it lists out running containers only. It is equivalent to docker ps. . . Highlighted Flags . Flag Description . --filter (or -f) | lists out containers that satisfy the specified filter. you need to specify key and (optionally) value for filtering. add multiple --filter flags for multiple conditions | . --all (or -a) | lists out all containers (both stopped and running containers) | . --quiet (or -q) | returns container IDs only. particularly useful when you want to propagate a list of container ID to another command | . --format | customizes the attributes you want to print out for the containers. | . --size | additionally show the file size for each containers. attribute Size is exposed for you to specify in --format. it is set as an additional flag because it is costly to query file size for each container. | . Examples . docker container ls --all size --format &quot;ID:{{.ID}} Image:{{.Image}} Size:{{.Size}}&quot; : print out all containers‚Äô IDs, image names and file sizes. note that {{.Size}} is only accessible with --size flag | docker container ls --all --filter &quot;ancestor=ubuntu&quot; --filter &quot;ancestor=python:3.8 --filter &quot;label=version=1.3&quot; -q: print out ID of all containers whose image is ubuntu:latest OR whose image is python:3.8 OR whose meta labels contain key version of value 1.3. | . docker container inspect &lt;container-id&gt; . You can pass one or more container names/ IDs to the command and it returns all metadata associated to those containers . . Highlighted Flags . Flag Description . --format (or -f) | functionally similar to --format from docker container ls, except you can access more attributes here. (e.g. NetworkSettings.Networks.bridge.IPAddress for container‚Äôs IP address) | . --size | additionally expose attribute SizeRootFs in the metadata. it expresses container‚Äôs file size in bytes. | . Examples . docker inspect --size --format &quot;{{.ID}} {{.Config.Image}} {{.SizeRootFs}} {{.NetworkSettings.Networks.bridge.IPAddress}}&quot; $(docker ps -a -q) : print out all containers‚Äô IDs, associated image names, file sizes (in bytes) and IP addresses. you can‚Äôt access SizeRootFs attribute without --size flag | . docker exec &lt;container-id&gt; &lt;command&gt; . Execute a command &lt;command&gt; in a running container &lt;container-id&gt; . . Highlighted Flags . Flag Description . --it | attach to and interact with the session in the container after the command execution adasdasds | . --user (or -u) | execute the command as a specified user (username or UID) | . Examples . docker exec -it -u 0 &lt;container-id&gt; bash: initiate a bash session as root user (UID 0) in a running container as root user | docker exec &lt;container-id&gt; pwd: print out the default directory of a running container | . Some Other Useful Docker Commands . docker start &lt;container-id&gt; . activate a stopped container &lt;container-id&gt; to run again . docker stop &lt;container-id&gt; . gracefully shut down a running container &lt;container-id&gt; . docker kill &lt;container-id&gt; . forcefully shut down a running container &lt;container-id&gt; . docker logs &lt;container-id&gt; . fetch the latest logs from &lt;container-id&gt;‚Äôs terminal for the command that it is running . docker cp &lt;src-path&gt; &lt;container-id&gt;:&lt;dest-path&gt; . transfer file from local host &lt;src-path&gt; to a directory in container &lt;dest-path&gt;. to transfer a file from container to local host, use docker cp &lt;container-id&gt;:&lt;src-path&gt; &lt;dest-path&gt; instead . docker container rm &lt;container-id&gt; . Delete the stopped container &lt;container-id&gt;. add --force (or -f) to forcefully stop a running container and then delete it. e.g. docker rm $(docker container ls --all --filter &quot;ancestor=ubuntu&quot; --quiet) deletes all containers from the image ubuntu:latest . 3. Customise Your Image with Dockerfile . . Sometimes you may want to containerisze your own service. If you don‚Äôt find any base images from Docker Hub with all required dependencies pre-intsalled, you can create your custom image. . The workflow for creating and running a custom image is straight forward. You start by writing your own Dockerfile. The file contains all configurations required to set up your container (e.g. commands to install required depencies, mounting, commands to start up the service ‚Ä¶ etc.). Once the Dockerfile is ready, you can build a custom image from your Dockerfile, and then spin off containers from it. . . Step 1: Configure Your Dockerfile . Here are the instructions you can specified in Dockerfile (some have their equivalent flags from docker run). Note that you can access environment variables by $&lt;env_var&gt; (or $(&lt;env_var&gt;)) in Dockerfile: . Instruction Description . FROM | base image that you want to build upon, you can specify a specific tag for the image (e.g. FROM python:3.8) | . RUN | command you want to run in the container, usually for commands that install dependencies (e.g. RUN pip install -r requirements.txt) | . WORKDIR | set a working directory for any instructions that follow. if relative path is used, it will be relative to the WORKDIR from latest instruction | . COPY | copy files or directory from local host to the container | . LABEL | attach meta label to the contaienr, equivalent to --label (or -l) in docker run (e.g. LABEL version=4.4 equivalent to docker ps --filter label=version=4.4) | . ENTRYPOINT | default command you call in the container. it can‚Äôt be override by docker run | . CMD | default command you call in the container, it can be overriden by the command appended to docker run. additionally when used with ENTRYPOINT, it serves as parameters to the command from ENTRYPOINT | . Here is an example for containerizing a data pipeline written in a Python script. . FROM sample-image:sample-tag RUN pip install pandas WORKDIR /working_dir WORKDIR working_subdir COPY pipeline.py pipeline.py LABEL version=&quot;1.3&quot; LABEL create_date=&quot;2022-02-01&quot; ENTRYPOINT [&quot;python&quot;, &quot;pipeline.py&quot;] CMD [&quot;2022-02-01&quot;] . Step 2: Build an image from your Dockerfile . Build a custom image of name &lt;image-name&gt; with tag &lt;tag&gt; from your Dockerfile: . docker build -t &lt;image-name&gt;:&lt;tag&gt; . . note that rerunning the command overwrites the previous image | . means reading Dockerfile in current directory | to check the image has been successfully built, do docker images to list out available images | . Step 3: Fire off a container from your built image . Once the image is built, you can spin off a container from it: . docker run &lt;image-name&gt;:&lt;tag&gt; . 4. Manage Your Containers Easily with Docker Compose . . Sometimes it could be a pain to manage a group of containers with manual commands. Docker Compose is a tool to simplify the process. You can configure the services in docker-compose.yml file, and then you can easily manage them (e.g. build images, spin off/ stop containers) with docker-compose commands. . As a side note, Kubernetes (aka k8s) is another popular alternative. It supports automatic deployment, scaling and management of containerized services. However, k8s is beyond the scope of this guide. . Let‚Äôs say you want to package a Postgres server and a pgAdmin server (GUI tool for interacting with Postgres database) as separate containers. On top of that, the containerized pgAdmin should be able to connect with the containerized Postgres server and the containerized pgAdmin GUI should be accessible from local host. You can do the steps below to manage the containers. . Step 1: Configure Your docker-compose.yml . docker-compose.yml defines all configurations you want to set for each service. Note that unlike docker run, we can use relative path in docker-compose.yml. . services: pgdatabase: image: postgres:13 environment: - POSTGRES_USER=root - POSTGRES_PASSWORD=root - POSTGRES_DB=ny_taxi volumes: - &quot;./ny_taxi_postgres_data:/var/lib/postgresql/data:rw&quot; ports: - &quot;5432:5432&quot; pgadmin: image: dpage/pgadmin4 environment: - PGADMIN_DEFAULT_EMAIL=admin@gmail.com - PGADMIN_DEFAULT_PASSWORD=root ports: - &quot;8080:80&quot; . Step 2: Bring Up Containers . Once docker-compose.yml is ready, you can run the following command in the same directory to bring up the containers. The command builds custom image and spin off container for each service: . docker-compose up . similar to docker run, you can attach --detach (or -d) for running the group of containers in detached mode | by default a custom bridge network will be created for the group of containers to enable inter-container communication by IP address or name resolution | unlike docker build, rerunning docker-compose up doesn‚Äôt rebuild an image even with updated docker-compose.yaml | to reflect your change in docker-compose.yaml, you can either: apply docker-compose up --build to enforce re-building, or equivalently | apply docker-compose build before docker-compose up | . | note that the container‚Äôs name will have additional prefix and suffix on top of what you specify in docker-compose.yml. the prefix is based on the name of the folder that you run docker-compose up. you can override the prefix by the flag --project-name or -p (i.e. docker-compose --project-name &lt;some-prefix&gt; up) | . Step 3: Bring Down Containers . Finally you can bring down what you have brought up from docker-compose up by: . docker-compose down . not only it shuts down the containers, it will also remove the associated custom bridge network and the containers | in case there are residual containers remained, you can apply docker-compose rm in the same directory to erase them | . 5. References . . Youtube: DE Zoomcamp 1.2.1 - Introduction to Docker | Docker Documentation: docker run | Docker Documentation: docker container ls | Github Issue: Docker PS filter by ancestor image does not match tagged images if image tag is omitted from filter ¬∑ Issue #24295 ¬∑ moby/moby | Youtube: Manage Docker Easily With VS Code |",
            "url": "https://riven314.github.io/alexlauwh314/de-zoomcamp/software%20engineering/2022/02/13/DockerSurvivalGuide.html",
            "relUrl": "/de-zoomcamp/software%20engineering/2022/02/13/DockerSurvivalGuide.html",
            "date": " ‚Ä¢ Feb 13, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Paper Summary: "NeRF++: Analyzing and Improving Neural Radiance Fields"",
            "content": ". 1. Motivations . . Thanks to over-parameterization, Neural Radiance Field (NeRF) has strong expressive freedom to describe a 3D scene‚Äôs geometry and its radiance. How is it possible to escape from numerous wrong solutions and converge to a correct one? . This is a question captivating authors‚Äô attention. It turns out the anwser lies on some choices of design on NeRF‚Äôs structure which implicitly impose a form of regularization on its learning. (I personally find this part irrelevant to their main proposal, but it does offer me a new perspective on NeRF). . Despite such amazing capacity, NeRF has difficulty to learn some kinds of scenes. Unbounded scene is one of them which the authors are trying to address. It has its unique challenges because its background is indispensable to render and objects can be anywhere in the scene. . Standard NeRF doesn‚Äôt work well for unbounded scenes because it encloses a scene by a finite volume where it draws samples for volumetric rendering. To address the problem, the authors propose NeRF++ to model foreground and background as 2 separate volumes. . 2.1. How Could NeRF Possibly Learn So Well? . . NeRF is a neural network which describes the geometry (i.e. density) and radiance (i.e. colour) of a 3D scene. By the law of Universal Approximation Theorem, network like this could approximate any continuous functions. With so much expressive freedom empowered by its design, NeRF could possibly go wrong in many ways to overfit to training set without generalizing to novel views, unless we have a dense set of training examples as strong supervision. . The authors did an experiment to illustrate the point. They forced NeRF to learn wrong geometry (i.e. surface of a unit sphere) while supervised its radiance on a set of posed views from lego scene. Despite the fact that its learned geometry is far from lego-like, it manged to explain well on views from training set. Not surprisingly the trained NeRF failed miserably to render any novel views from test set. . . This study suggests that NeRF‚Äôs expressive capacity is clearly overpowered, to a degree that even if it screws up on geometry, it could still fit perfectly to the training views. The authors call this phenomenon ‚Äúshape-radiance ambiguities‚Äù. . But unexpectedly many empirical studies show that NeRF could typically learn the correct geometry and radiance of a 3D scene. It is therefore captivating to ask: How on earth could NeRF learn so well in spite of so much freedom to go wrong? . 2.2. The Answer Lies On Its Design! . . It turns out the sanity of NeRF has been secretly safeguarded by some choices of design in its model structure, which implicitly serve as a form of regularisation against its learning. The role of these decisions on model was rarely discussed in previous literatures, and this paper finally takes the chance to reveal it. . The authors conjecture 2 possible decisions on NeRF‚Äôs model structure that drives it towards correct geometry and radiance at high chance: . 1. Low-frequency Encoding on View Direction . One trick applied by original NeRF is to project both viewing direction $( theta, phi)$ and spatial coordinate $(x, y, z)$ into high dimensional space by positional encoding. The encoding is independently cast on each parameter. We could formalize the encoding as $ gamma^{L}(p)$, where $L$ is a hyper-parameter that controls the dimension of its output and input $p$ is bounded by $[-1, 1]$: . Œ≥L(p)=(sin(20œÄp),cos(20œÄp),...,sin(2L‚àí1œÄp),cos(2L‚àí1œÄp)) begin{split} . gamma^{L}(p) = (sin(2^{0} pi p), cos(2^{0} pi p), ‚Ä¶, sin(2^{L-1} pi p), cos(2^{L-1} pi p)) . end{split}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;Œ≥L(p)=(sin(20œÄp),cos(20œÄp),‚Ä¶,sin(2L‚àí1œÄp),cos(2L‚àí1œÄp))‚Äã&lt;/span&gt;&lt;/span&gt; . Note that $sin(2^{L-1} pi p)$ or $cos(2^{L-1} pi p)$ has higher frequency with increasing $L$, so higher $L$ does not only contribute to higher dimensional space but also higher frequency variation on the output. As a result, positional encoding is conducive for NeRF to learn high-frequency details of a scene. To illustrate the impact of positional encoding on rendering quality, Ankur Handa has made an excellent visual comparison: . This is a great blog that provides intuitive explanation of the positional encoding used in transformers https://t.co/x8Mse71ARPUnlike RNNs transformers do not have any notion of word order so this positional encoding helps in capturing the order of words in sequence. pic.twitter.com/2IKJMamNE5 . &mdash; Ankur Handa (@ankurhandos) April 19, 2020 Now comes the hidden details - viewing direction is encoded by $ gamma^{4}(p)$ while spatial coordinate is encoded by $ gamma^{10}(p)$. This implies that features encoded from viewing direction is not so sensitive to high frequency variation compared against that from spatial coordinate. This favors the view-dependent radiance to be smooth and of low frequency with respect to the change in viewing direction. . Incorrect geometry has to be compensated by high-frequency radiance to explain well on training samples. Such low-frequency encoding on viewing direction makes NeRF struggle on high-frequency radiance, and hence guide it towards correct geometry. . 2. Viewing Direction Injected at the Near End of Network . The more layers an input feature has passed through, the more complex pattern it could expresse thanks to compounded compositions of non-linear transformations. While spatial coordinate is fed as an input at the beginning of the network, viewing direction is only appended at almost the end. . This limits the complexity that viewing direction could express as features. But on the flip side, it actually serves as a form of regularization to prevent the view-dependent radiance from going too wild. . . 3.1. NeRF Shortcoming for Modelling Unbounded Scene . . Having said NeRF comes with regularization in its structure, it is not without its shortcomings. One issue that the authors is trying to address is its limitation to model unbounded scenes. . Previous studies typically experimented NeRF on bounded scenes that are properly controlled, where an object of interest is centered at a stage and the posed views are taken at roughly a fixed distance from the object. Synthetic Dataset is one of the typical benchmark datasets in point. Such controlled environment makes NeRF easier to learn the scene, but they are far from any representations of scenes we would encounter in real life. . . In practice scenes are unbounded - there could be more than one object of interest and objects could be anywhere in the scene. In particular, background is an indispensable part of the scene so a model should preserve fidelity of both foreground and background in order to be representative of an unbounded scene. . . All these conditions impose significant challenges to standard NeRF. It is caused by the setting that during volumetric rendering standard NeRF samples 3D points from a packed volume. It works well for controlled environment because we know where the object lies, but for unbounded scenes the volume is unlikely to capture most objects from background. The consequence is a blurry background thanks to insufficient sampling of objects from background. . One remedy is to extend the length of the interval in a hope to cover most objects for sampling, but doing so has several practical limitations. If we fix the number of samples per traced ray, the rendered would look blurry in overall because of insufficient samples assigned to both foreground and background. But if we increase the number of samples, it significantly ramps up the computational cost which makes both training and rendering much more time-consuming. On top of that, it would be numerically unstable to evaluate points at far distance from the viewer. . To validate the claim, the authors have shown from experiment that standard NeRF struggles to render views of high fidelity in an unbounded Truck scene. . . 3.2. Parameterising Background as an Inverted Sphere . . The authors propose NeRF++ to address the issue. Its model structure is the same as standard NeRF, except that it segregates foreground and background of a scene into 2 separate networks: . Inner NeRF learns the foreground enclosed by a volume of unit sphere centered at origin | Outer NeRF learns the background enclosed by a volume of its complement (i.e. an inverted sphere). | . With such segregation, volumetric rendering has to take into account constitutes from both foreground and background along a traced ray in order to render colour for a pixel $ bold{C}( bold{r})$. If we denote $t‚Äô$ as the time that the traced ray crosses the boundary between the two volumes, its theoretical formulation can expressed as follows: . C(r)=‚à´t=0t‚Ä≤œÉ(o+td)‚ãÖc(o+td,d)‚ãÖe‚àí‚à´s=0tœÉ(o+sd)dsdt+e‚àí‚à´s=0t‚Ä≤œÉ(o+sd)ds‚à´t=t‚Ä≤‚àûœÉ(o+td)‚ãÖc(o+td,d)‚ãÖe‚àí‚à´s=t‚Ä≤tœÉ(o+sd)dsdt begin{split} . bold{C}( bold{r}) &amp; = int^{t&#39;}{t=0} sigma( bold{o}+t bold{d}) cdot bold{c}( bold{o}+t bold{d}, bold{d}) cdot e^{- int^{t}{s=0} sigma( bold{o}+s bold{d})ds} dt &amp; + e^{- int^{t&#39;}{s=0} sigma( bold{o}+s bold{d})ds} int^{ infty}{t=t&#39;} sigma( bold{o}+t bold{d}) cdot bold{c}( bold{o}+t bold{d}, bold{d}) cdot e^{- int^{t}_{s=t&#39;} sigma( bold{o}+s bold{d})ds} dt . end{split}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;C(r)‚Äã=‚à´t=0t‚Ä≤‚ÄãœÉ(o+td)‚ãÖc(o+td,d)‚ãÖe‚àí‚à´s=0t‚ÄãœÉ(o+sd)dsdt+e‚àí‚à´s=0t‚Ä≤‚ÄãœÉ(o+sd)ds‚à´t=t‚Ä≤‚àû‚ÄãœÉ(o+td)‚ãÖc(o+td,d)‚ãÖe‚àí‚à´s=t‚Ä≤t‚ÄãœÉ(o+sd)dsdt‚Äã&lt;/span&gt;&lt;/span&gt; . The former term is numerical rendering of foreground and the latter term is that of background. . One caveat that deviates NeRF++ from the above formula is that outer NeRF receives different inputs than inner NeRF. Same as standard NeRF, inner NeRF is parameterized to return density $ sigma_{in}( bold{x})$ and colour $ bold{c}_{in}( bold{x}, bold{d})$ for a spatial coordinate $ bold{x}$ at viewing direction $ bold{d}$. However, outer NeRF is parameterized slightly different: instead of treating target coordinate $ bold{x}$ as input, it firsly projects $ bold{x}$ onto the unit sphere to yield coordinate $ bold{x‚Äô} = (x‚Äô, y‚Äô, z‚Äô)$. And then it takes $ bold{x‚Äô}$ and the inverse of target coordinate‚Äôs distance $1/r$ as input. As a result, its outputs are parameterized as $ sigma_{out}( bold{x‚Äô}, 1/r)$ and $ bold{c}_{out}( bold{x‚Äô}, 1/r, bold{d})$. . I personally find such parameterization clever because it skillfully normalises an unbounded spatial coordinate to bounded quantities. Such normalisation helps network to learn the background scene more efficiently. In addition, transforming unbounded inputs to bounded ones help avoid numerical unstability in optimization. . 3.3. Finding $(x‚Äô, y‚Äô, z‚Äô)$ from $1/r$ . . I think it is worth some space to explain how we could find $(x‚Äô, y‚Äô, z‚Äô)$ and $1/r$. . Let‚Äôs say we have a traced ray $ bold{r}(t) = bold{o} + t bold{d}$ marching from the inner volume through the outer volume. We start off by randomly sample $1/r$ from the interval $(0, 1)$. The value of $r$ (note that this $r$ is different from the traced ray $ bold{r}(t)$!) should determine the target coordinate $ bold{x}$ that we want: it is the point that lies on the traced ray at a distance of $r$ from the origin. With $ bold{x}$ we could therefore find its projection onto the unit sphere. . While the above intuition should work in principle, there is a simpler approach to find $(x‚Äô, y‚Äô, z‚Äô)$, as follows: . Let $ bold{b} = bold{o} + t_{b} bold{d}$ to be the vector that is perpendicular to the traced ray. Find $ bold{b}$ by solving the equation $ bold{d} cdot ( bold{o}+t_{b} bold{d}) = 0$ for $t_{b}$ | Let $ bold{a} = bold{o} + t_{a} bold{d}$ to be the vector intersecting the unit sphere and the traced ray. Find $ bold{a}$ by solving the equation $ Vert bold{o} + t_{a} bold{d} Vert = 1 $ for $t_{a}$ | Let $ theta$ be the angle between $ bold{a}$ and the traced ray. Find $ theta$ by solving $sin( theta) = Vert bold{b} Vert$ | Let $ phi$ be the angle between the target coordinate $ bold{x}$ and the traced ray. Find $ phi$ by solving $sin( phi) = Vert bold{b} Vert / r$ | Finally we get $ omega = theta - phi$. This is the clockwise rotational angle along the plane $ bold{b} times bold{a}$ that we can apply to $ bold{a}$ in order to make it aligned to the target projection $(x‚Äô, y‚Äô, z‚Äô)$! One merit of this approach is that when $r$ is updated, we only need to simply update $ phi$ and $ omega$ in order to apply another round of angular rotation on $ bold{a}$! . The geometry of this approach can be neatly visualized below. Note that I draw the geometry in 2D (instead of 3D) just for the convenience of illustration: . . 4. Experimental Results . . NeRF++ managed to render views that look sharp in both foreground and background. In comparison, background rendered by NeRF looks particularly blurry. Below are some experimental results for the scenes taken from Tanks and Temples benchmark dataset: . . The diagram below shows that the proposed parameterisation done by NeRF++ has faithfully segregated foreground from background, without sacrificing the overall fidelities of the Truck scene. . . Below are quantitative comparison on the same dataset. We can clearly notice that NeRF++ significantly outperforms NeRF for all scenes and all metrics. . As a remark, LPIPS stands for Learned Perceptual Image Patch Similarity which captures differences between image patches (lower means better) and SSIM stands for Structural Similarity, which captures perceptual similarity between 2 images. . . 5. Limitations . . Higher Computational Cost: NeRF++ is more costly on training and rendering than standard NeRF. It takes ~24 hours with 4 RTX 2080 Ti GPUs to train NeRF++ and ~30 seconds to render a single 1280x720 image. | Ignorance to Photometric Effect: Though not a issue specific to NeRF++, it doesn‚Äôt take into account unexpected photometric effects such as auto-exposure, vignetting caused by a camera. These effects may contaminate the training samples because it leads to views of different colours even at the same pose, which breaks an assumption imposed on NeRF: same colour should be rendered at same pose. | . 6. References . . NeRF++: Analyzing and Improving Neural Radiance Fields | Vladlen Koltun: Towards Photorealism (September 2020) | NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis |",
            "url": "https://riven314.github.io/alexlauwh314/paper/computer-graphics/2022/02/01/NeRF++.html",
            "relUrl": "/paper/computer-graphics/2022/02/01/NeRF++.html",
            "date": " ‚Ä¢ Feb 1, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "Paper Summary: "PlenOctrees for Real-time Rendering of Neural Radiance Fields"",
            "content": ". 1. Motivations . . Neural Radiance Field (NeRF) has gained traction in academia thanks to its power to render novel 2D views of a 3D scene trained on the posed images. However NeRF‚Äôs rendering speed is noticeably slow. Without a GPU accelerator, it could easily take more than 1 minute to render a 2D view. Such drawback blocks its application on latency-sensitive domain. . It is slow becaues numerous network calls have to be made for rendering even one single pixel. The authors of this paper propose the following to address this issue and achieves a significant speedup: . Invent an efficient data structure called PlenOctrees to spatially cache the outputs from a trained NeRF | Introduce a variant of NeRF (NeRF-SH) that is easily convertible into PlenOctree representation | . 2. Introduction to Spherical Harmonics . . Spherical Harmonics (SH) is a critical concept behind their proposal. Shperical Harmonics is a collections of spherical functions used to describe the surface of some special spheres. . Technically, each of the function receives a direction in 3D space, parametrised as $( theta, phi)$, as an input and the absolute value of its output (output is a complex number) tells you the associated surface‚Äôs distance from the origin. The functions are usually annotated as $Y_{l}^{m}( theta, phi)$ where $m$ and $l$ determines the shape of the sphere to be described. . Below visualise some instances of the shperical surfaces: . . In the same sprit that some functions can be decomposed into polynomials, and that any periodic functions can be decomposed into Fourier Series, Spherical Harmonics are powerful enough to express any spherical function when composed together properly. . Additionally it provides a compact way to represent any spherical functions. We only need to book keep the coefficients for each SH function and then we can recover the target spherical function at any input (details in next session). . How could we apply this concept to NeRF? Recall that NeRF describes the geometry (i.e. density $ sigma$) and appearance (i.e. RGB colour $ bold{c}$) of any 3D point $ bold{x}$ in a 3D scene at any viewing direction $ bold{d}$. While a 3D point‚Äôs density is invariant of your viewing direction, its colour varies with your viewing angle. Therefore, we can treat the colour exactly like spherical function, except that we need 3 independent real-valued spherical functions to do so because we need 3 channels (red, green and blue) to describe a RGB colour. . 3.1. NeRF-SH: Linking Spherical Harmonics to NeRF . . The authors propose NeRF-SH that models a 3D point‚Äôs colour as spherical fucntions. NeRF-SH receives a 3D point $ bold{x}$ as input and predicts its density $ sigma$ the same way as vanilla NeRF does. But instead of predicting its RGB colour for a single viewing direction, NeRF-SH predicts spherical functions that describes the colours at all viewing direction. This modification helps factor out viewing direction from the network input. It enables a space-efficient way to spatially cache the colours predicted by the network (into a PlenOctree structure). . . We can compactly express a spherical function as coefficients of Spherical Harmonics $(k_{l}^{m})_{l:0 leq l leq l_{max}}^{m: -l leq m leq l}$, where $k_{l}^{m}$ is 3-dimensional for describing the 3 channels of RGB colour. . Once we yield the coefficients $(k_{l}^{m})_{l:0 leq l leq l_{max}}^{m: -l leq m leq l}$, we could easily render the colour at any viewing direction by a sum of Spherical Harmonics $ bold{Y}_{l}^{m}( theta, phi)$ weighted by their associated $k_{l}^{m}$, followed by a sigmoid transformation $S(.)$: . . We can neatly summarize the pipeline for training NeRF-SH with a diagram extracted from the paper. . . 3.2. NeRF-SH: Training with Sparsity Loss . . There is an additional caveat for the training: when solely supervised by standard reconstruction loss during training, NeRF-SH tends to predict arbitrary geometry on unused region. Although it doesn‚Äôt hurt the rendering quality but it occupies a lot of redundant space when NeRF-SH is converted into PlenOctree. . To encourage NeRF-SH to predict unused region to be empty, a sparsity loss is additionally enforced. To evaluate the loss, we uniformly sample $K$ points within a bounding box and consider their associated densities $ sigma_{k}$. High density leads to high sparsity loss. . . With sparsity loss enforced, the unused region can be effectively pruned away from PlenOctree thanks to their negligible densities. As a result, it leads to a tighter bound on a 3D scene and hence a higher spatial resolution on PlenOctree representation (because voxel cells are mostly distributed to the important region). . . 4.1. Conversion to PlenOctree . . Once NeRF-SH is trained, we can easily converted it into a PlenOctree representation with the following procedures: . Evaluation: Sample uniformly spaced 3D grid points and evaluate their density $ sigma$ with the trained NeRF-SH. | Filtering: Partition the grid points into different voxel cells. Render all training views and keep track of the maximum ray weight $1-exp(- sigma_{i} delta_{i})$ in each voxel cell, where $ delta_{i}$ is the distance between sample points along a ray (more details in next session). Cell with low ray weight implies it is likely an empty space with negligible contribution to any training views so we can safely prune them. | Sampling: To determine the SH coefficients for each voxel cell, we randomly sample 256 points within a voxel cells and take an average of their associated SH coefficients. | 4.2. Rendering with PlenOctree . . With the converted PlenOctree, we could easily achieve blazingly fast rendering speed with the following procedures: . Cast a ray $ bold{r}$ from the pixel to be rendered into the PlenOctree and consider all voxel cells that intersect with the ray. | Segment the ray by voxel boundaries. Consider the lengths of each segments as ${ delta_{i}}_{i=1}^{N}$ and look up their associated density ${ sigma_{i} }_{i=1}^{N}$ and SH coefficients ${ bold{k_{i}} }_{i=1}^{N}$ from each voxel cell. Colour of each segment ${ c_{i} }_{i=1}^N$ can be found with the spherical functions represented by the SH coefficients. | With the above quantities, apply standard volumetric rendering formula to render colour $ hat{C}( bold{r})$ at the target pixel. | We could achieve further speedup by early stopping the ray when its accumulated transmittance $T_{i}$ is too low. The accumulated transmittance indicates the chance that the ray can pass through the first segment up to the target segments without being blocked. Points with low accumulated transmittance have negligible impact to the rendered colour so we could safely skip them. | . The whole process is similar to that of NeRF except PlenOctree makes use of cache from its voxel cells so it can achieve significant speedup comparing against a standard NeRF. . We can summarise the rendering process with a diagram below. $Y(.)$ is the SH functions as a vector and $ bold{k}_{i}$ are the SH coefficients associated to the $i$-th voxel cell. $c_{i}$ is its associated colour and the formula is no different from what we discussed before. . . 4.3. Fine-tuning PlenOctree . . Since the rendering process with PlenOctree is done by standard volumetric rendering, the operation is differentiable. Therefore, we could apply stochastic gradient descent to fine-tune the PlenOctree representation. Empirical studies show that additional fine-tuning on PlenOctree could lead to significant improvement on rendering quality. . While in principle it is feasible to train a PlenOctree representation from scratch, it usually takes much more time to converge. A trained NeRF-SH gives PlenOctree a good prior of geometry and appearance to learn from. . We can neatly summarised the pipeline for yielding a PlenOctree representation with the following diagram extracted from the paper. . . 5. Experimental Results . . The authors compares both the rendering speed and quality between their approach (NeRF-SH + PlenOctree) and existing models. The rendering speed is measured by Frames per Second (FPS) and the rendering quality is measured by Peak Signal-to-Noise Ratio (PSNR). . They experimented their approach with different settings (e.g. higher filtering threshold on accumulated transmittance, reduce grid size to 256) and found that a few of its settings (i.e. Ours-1.9G) achieved better PSNR and 3000x faster FPS against vanilla NeRF! . . 6. Limitations . . While NeRF-SH and PlenOctree achieves amazing rendering speed with comparable rendering quality against standard NeRF, it has several trade-offs. . On one hand, it has larger memory footprint than a standard NeRF. A NeRF model is typically light-weighted with roughly ~10 MB. However, a PlenOctree representation could easily take up to ~2 GB because it caches density and SH coefficients in each voxel cell. . On the other hand, it creates noticeable artifact when you zoom in the scene because it partitions a continuous 3D space into discrete voxel cells. The resolution is inevitably sacraficed. . . 7. References . . PlenOctrees for Real-time Rendering of Neural Radiance Fields | Real-time rendering of NeRFs with PlenOctrees - Angjoo Kanazawa |",
            "url": "https://riven314.github.io/alexlauwh314/paper/computer-graphics/2022/01/26/PlenOctree-Summary.html",
            "relUrl": "/paper/computer-graphics/2022/01/26/PlenOctree-Summary.html",
            "date": " ‚Ä¢ Jan 26, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.‚Ü© . 2. This is the other footnote. You can even have a link!‚Ü© .",
            "url": "https://riven314.github.io/alexlauwh314/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " ‚Ä¢ Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "My name is Alex Lau. I am born and raised in Hong Kong üá≠üá∞. I received my undergraduate (Actuarial Science) and master (Data Science) degree at The University of Hong Kong. I am currently a Software Engineer at Aicadium. Before that, I was an AI Software Engineer at Pencil. . In my free time, I like reading papers, blogging üìì, contributing to open-source projects, coding on Kaggle üë®‚Äçüíª and running üèÉ. I love collaborations with like-minded researchers and data professionals on interesting side projects related to Machine Learning or Quantitative Trading. Do reach out to me if you have interesting ideas! . Contact me . alexlauwh@gmail.com .",
          "url": "https://riven314.github.io/alexlauwh314/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://riven314.github.io/alexlauwh314/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}