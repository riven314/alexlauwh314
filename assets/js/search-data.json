{
  
    
        "post0": {
            "title": "Survival Guide for Docker & Docker Compose",
            "content": ". 1. Who Should Read This? . . You probably know what Docker is and how they are commonly used in practice, but are looking for a quick guide to navigate through the jungle. This article summarises the basics in Docker and Docker Compose to help you get started. . This is originally a note for Data Engineering Zoomcamp lesson 1, and over time I expanded it with additional references. Now I decided to package it as a stand-alone survival guide with a hope to help those in my situation, who intend to quickly get themselves up for this amazing tools. . 2. Essential Docker Commands . . docker run &lt;image-name&gt;:&lt;tag&gt; &lt;command&gt; . The command does the following in order: . look up target image &lt;image-name&gt; of specified tag &lt;tag&gt; locally, otherwise download it from Docker Hub (if &lt;tag&gt; is not specified, it assumes the tag to be latest) | spin off a container from the downloaded image | execute &lt;command&gt; in the running container, if provided | gracefully stop the container after the &lt;command&gt; finishes | Remarks: . each execution of the command spins off independent container | absolute path should be used if a path is needed in docker run | . . Highlighted Flags . Flag Description . --detach (or -d) | run the container in background | . --rm | delete the container at the end of the command. useful if you intend to run the container once only | . --name | specify a name to your container. useful for inter-container communication because other containers could address your container by its name instead of its IP address. | . --env (or -e) | set an environment variable in the container. useful when the service to be containerized requires environment variables, repeatedly add --env for multiple environment variables. | . -it | attach to the session in the container at the end and enable you to interact with the session, useful when you have appended command and it | . --label (or -l) | add a meta label to the container. it’s useful if you want to group containers by labels, same as --env you can repeatedly add --label for multiple meta labels | . --volume (or -v) | mount a directory in localhost to a directory in the container e.g. --volume &lt;local-directory&gt;:&lt;container-directory&gt; | . --expose (or -p) | binds a port in local host to a port in container. useful for localhost to communicate with the containerized service e.g. --expose &lt;local-port&gt;:&lt;container-port&gt; | . --entrypoint | run specified commands in the container, functionally similar to &lt;command&gt; in this case. | . Examples . docker run python:3.8 ls: spin off a container from image python:3.8 and then list the folders &amp; files at default directory of the container | docker run --rm -d ython:3.8 python -m http.server: start a http server at default directory of a container and run the container in background (running in background is useful here, otherwise your terminal session can’t resume unless you forcefully shut down the server). upon shutting down the container, it is automatically deleted (thanks to --rm). | docker run -it ubuntu bash: spin off a container from ubuntu:latest image. execute bash in the container and then attach to the associated bash session (thanks to -it). | docker run -it -e POSTGRES_USER=&quot;root&quot; -e POSTGRES_PASSWORD=&quot;root&quot; -e POSTGRES_DB=&quot;ny_taxi&quot; -p 5432:5432 postgres:13: spin off a container from postgres:13 image and set 3 environment variables for the postgres database in the container. additionally the container exposes its port 5432 to local host’s port 5432. then the container starts postgres server and we attach to the associated session. | . docker container ls . This command lists out attributes for each container, by default it lists out running containers only. It is equivalent to docker ps. . . Highlighted Flags . Flag Description . --filter (or -f) | lists out containers that satisfy the specified filter. you need to specify key and (optionally) value for filtering. add multiple --filter flags for multiple conditions | . --all (or -a) | lists out all containers (both stopped and running containers) | . --quiet (or -q) | returns container IDs only. particularly useful when you want to propagate a list of container ID to another command | . --format | customizes the attributes you want to print out for the containers. | . --size | additionally show the file size for each containers. attribute Size is exposed for you to specify in --format. it is set as an additional flag because it is costly to query file size for each container. | . Examples . docker container ls --all size --format &quot;ID:{{.ID}} Image:{{.Image}} Size:{{.Size}}&quot; : print out all containers’ IDs, image names and file sizes. note that {{.Size}} is only accessible with --size flag | docker container ls --all --filter &quot;ancestor=ubuntu&quot; --filter &quot;ancestor=python:3.8 --filter &quot;label=version=1.3&quot; -q: print out ID of all containers whose image is ubuntu:latest OR whose image is python:3.8 OR whose meta labels contain key version of value 1.3. | . docker container inspect &lt;container-id&gt; . You can pass one or more container names/ IDs to the command and it returns all metadata associated to those containers . . Highlighted Flags . Flag Description . --format (or -f) | functionally similar to --format from docker container ls, except you can access more attributes here. (e.g. NetworkSettings.Networks.bridge.IPAddress for container’s IP address) | . --size | additionally expose attribute SizeRootFs in the metadata. it expresses container’s file size in bytes. | . Examples . docker inspect --size --format &quot;{{.ID}} {{.Config.Image}} {{.SizeRootFs}} {{.NetworkSettings.Networks.bridge.IPAddress}}&quot; $(docker ps -a -q) : print out all containers’ IDs, associated image names, file sizes (in bytes) and IP addresses. you can’t access SizeRootFs attribute without --size flag | . docker exec &lt;container-id&gt; &lt;command&gt; . Execute a command &lt;command&gt; in a running container &lt;container-id&gt; . . Highlighted Flags . Flag Description . --it | attach to and interact with the session in the container after the command execution adasdasds | . --user (or -u) | execute the command as a specified user (username or UID) | . Examples . docker exec -it -u 0 &lt;container-id&gt; bash: initiate a bash session as root user (UID 0) in a running container as root user | docker exec &lt;container-id&gt; pwd: print out the default directory of a running container | . Some Other Useful Docker Commands . docker start &lt;container-id&gt; . activate a stopped container &lt;container-id&gt; to run again . docker stop &lt;container-id&gt; . gracefully shut down a running container &lt;container-id&gt; . docker kill &lt;container-id&gt; . forcefully shut down a running container &lt;container-id&gt; . docker logs &lt;container-id&gt; . fetch the latest logs from &lt;container-id&gt;’s terminal for the command that it is running . docker cp &lt;src-path&gt; &lt;container-id&gt;:&lt;dest-path&gt; . transfer file from local host &lt;src-path&gt; to a directory in container &lt;dest-path&gt;. to transfer a file from container to local host, use docker cp &lt;container-id&gt;:&lt;src-path&gt; &lt;dest-path&gt; instead . docker container rm &lt;container-id&gt; . Delete the stopped container &lt;container-id&gt;. add --force (or -f) to forcefully stop a running container and then delete it. e.g. docker rm $(docker container ls --all --filter &quot;ancestor=ubuntu&quot; --quiet) deletes all containers from the image ubuntu:latest . 3. Customise Your Image with Dockerfile . . Sometimes you may want to containerisze your own service. If you don’t find any base images from Docker Hub with all required dependencies pre-intsalled, you can create your custom image. . The workflow for creating and running a custom image is straight forward. You start by writing your own Dockerfile. The file contains all configurations required to set up your container (e.g. commands to install required depencies, mounting, commands to start up the service … etc.). Once the Dockerfile is ready, you can build a custom image from your Dockerfile, and then spin off containers from it. . . Step 1: Configure Your Dockerfile . Here are the instructions you can specified in Dockerfile (some have their equivalent flags from docker run). Note that you can access environment variables by $&lt;env_var&gt; (or $(&lt;env_var&gt;)) in Dockerfile: . Instruction Description . FROM | base image that you want to build upon, you can specify a specific tag for the image (e.g. FROM python:3.8) | . RUN | command you want to run in the container, usually for commands that install dependencies (e.g. RUN pip install -r requirements.txt) | . WORKDIR | set a working directory for any instructions that follow. if relative path is used, it will be relative to the WORKDIR from latest instruction | . COPY | copy files or directory from local host to the container | . LABEL | attach meta label to the contaienr, equivalent to --label (or -l) in docker run (e.g. LABEL version=4.4 equivalent to docker ps --filter label=version=4.4) | . ENTRYPOINT | default command you call in the container. it can’t be override by docker run | . CMD | default command you call in the container, it can be overriden by the command appended to docker run. additionally when used with ENTRYPOINT, it serves as parameters to the command from ENTRYPOINT | . Here is an example for containerizing a data pipeline written in a Python script. . FROM sample-image:sample-tag RUN pip install pandas WORKDIR /working_dir WORKDIR working_subdir COPY pipeline.py pipeline.py LABEL version=&quot;1.3&quot; LABEL create_date=&quot;2022-02-01&quot; ENTRYPOINT [&quot;python&quot;, &quot;pipeline.py&quot;] CMD [&quot;2022-02-01&quot;] . Step 2: Build an image from your Dockerfile . Build a custom image of name &lt;image-name&gt; with tag &lt;tag&gt; from your Dockerfile: . docker build -t &lt;image-name&gt;:&lt;tag&gt; . . note that rerunning the command overwrites the previous image | . means reading Dockerfile in current directory | to check the image has been successfully built, do docker images to list out available images | . Step 3: Fire off a container from your built image . Once the image is built, you can spin off a container from it: . docker run &lt;image-name&gt;:&lt;tag&gt; . 4. Manage Your Containers Easily with Docker Compose . . Sometimes it could be a pain to manage a group of containers with manual commands. Docker Compose is a tool to simplify the process. You can configure the services in docker-compose.yml file, and then you can easily manage them (e.g. build images, spin off/ stop containers) with docker-compose commands. . As a side note, Kubernetes (aka k8s) is another popular alternative. It supports automatic deployment, scaling and management of containerized services. However, k8s is beyond the scope of this guide. . Let’s say you want to package a Postgres server and a pgAdmin server (GUI tool for interacting with Postgres database) as separate containers. On top of that, the containerized pgAdmin should be able to connect with the containerized Postgres server and the containerized pgAdmin GUI should be accessible from local host. You can do the steps below to manage the containers. . Step 1: Configure Your docker-compose.yml . docker-compose.yml defines all configurations you want to set for each service. Note that unlike docker run, we can use relative path in docker-compose.yml. . services: pgdatabase: image: postgres:13 environment: - POSTGRES_USER=root - POSTGRES_PASSWORD=root - POSTGRES_DB=ny_taxi volumes: - &quot;./ny_taxi_postgres_data:/var/lib/postgresql/data:rw&quot; ports: - &quot;5432:5432&quot; pgadmin: image: dpage/pgadmin4 environment: - PGADMIN_DEFAULT_EMAIL=admin@gmail.com - PGADMIN_DEFAULT_PASSWORD=root ports: - &quot;8080:80&quot; . Step 2: Bring Up Containers . Once docker-compose.yml is ready, you can run the following command in the same directory to bring up the containers. The command builds custom image and spin off container for each service: . docker-compose up . similar to docker run, you can attach --detach (or -d) for running the group of containers in detached mode | by default a custom bridge network will be created for the group of containers to enable inter-container communication by IP address or name resolution | unlike docker build, rerunning docker-compose up doesn’t rebuild an image even with updated docker-compose.yaml | to reflect your change in docker-compose.yaml, you can either: apply docker-compose up --build to enforce re-building, or equivalently | apply docker-compose build before docker-compose up | . | note that the container’s name will have additional prefix and suffix on top of what you specify in docker-compose.yml. the prefix is based on the name of the folder that you run docker-compose up. you can override the prefix by the flag --project-name or -p (i.e. docker-compose --project-name &lt;some-prefix&gt; up) | . Step 3: Bring Down Containers . Finally you can bring down what you have brought up from docker-compose up by: . docker-compose down . not only it shuts down the containers, it will also remove the associated custom bridge network and the containers | in case there are residual containers remained, you can apply docker-compose rm in the same directory to erase them | . 5. References . . Youtube: DE Zoomcamp 1.2.1 - Introduction to Docker | Docker Documentation: docker run | Docker Documentation: docker container ls | Github Issue: Docker PS filter by ancestor image does not match tagged images if image tag is omitted from filter · Issue #24295 · moby/moby | Youtube: Manage Docker Easily With VS Code |",
            "url": "https://riven314.github.io/alexlauwh314/de-zoomcamp/software%20engineering/2022/02/13/DockerSurvivalGuide.html",
            "relUrl": "/de-zoomcamp/software%20engineering/2022/02/13/DockerSurvivalGuide.html",
            "date": " • Feb 13, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Paper Summary: "NeRF++: Analyzing and Improving Neural Radiance Fields"",
            "content": ". 1. Motivations . . Thanks to over-parameterization, Neural Radiance Field (NeRF) has strong expressive freedom to describe a 3D scene’s geometry and its radiance. How is it possible to escape from numerous wrong solutions and converge to a correct one? . This is a question captivating authors’ attention. It turns out the anwser lies on some choices of design on NeRF’s structure which implicitly impose a form of regularization on its learning. (I personally find this part irrelevant to their main proposal, but it does offer me a new perspective on NeRF). . Despite such amazing capacity, NeRF has difficulty to learn some kinds of scenes. Unbounded scene is one of them which the authors are trying to address. It has its unique challenges because its background is indispensable to render and objects can be anywhere in the scene. . Standard NeRF doesn’t work well for unbounded scenes because it encloses a scene by a finite volume where it draws samples for volumetric rendering. To address the problem, the authors propose NeRF++ to model foreground and background as 2 separate volumes. . 2.1. How Could NeRF Possibly Learn So Well? . . NeRF is a neural network which describes the geometry (i.e. density) and radiance (i.e. colour) of a 3D scene. By the law of Universal Approximation Theorem, network like this could approximate any continuous functions. With so much expressive freedom empowered by its design, NeRF could possibly go wrong in many ways to overfit to training set without generalizing to novel views, unless we have a dense set of training examples as strong supervision. . The authors did an experiment to illustrate the point. They forced NeRF to learn wrong geometry (i.e. surface of a unit sphere) while supervised its radiance on a set of posed views from lego scene. Despite the fact that its learned geometry is far from lego-like, it manged to explain well on views from training set. Not surprisingly the trained NeRF failed miserably to render any novel views from test set. . . This study suggests that NeRF’s expressive capacity is clearly overpowered, to a degree that even if it screws up on geometry, it could still fit perfectly to the training views. The authors call this phenomenon “shape-radiance ambiguities”. . But unexpectedly many empirical studies show that NeRF could typically learn the correct geometry and radiance of a 3D scene. It is therefore captivating to ask: How on earth could NeRF learn so well in spite of so much freedom to go wrong? . 2.2. The Answer Lies On Its Design! . . It turns out the sanity of NeRF has been secretly safeguarded by some choices of design in its model structure, which implicitly serve as a form of regularisation against its learning. The role of these decisions on model was rarely discussed in previous literatures, and this paper finally takes the chance to reveal it. . The authors conjecture 2 possible decisions on NeRF’s model structure that drives it towards correct geometry and radiance at high chance: . 1. Low-frequency Encoding on View Direction . One trick applied by original NeRF is to project both viewing direction $( theta, phi)$ and spatial coordinate $(x, y, z)$ into high dimensional space by positional encoding. The encoding is independently cast on each parameter. We could formalize the encoding as $ gamma^{L}(p)$, where $L$ is a hyper-parameter that controls the dimension of its output and input $p$ is bounded by $[-1, 1]$: . γL(p)=(sin(20πp),cos(20πp),...,sin(2L−1πp),cos(2L−1πp)) begin{split} . gamma^{L}(p) = (sin(2^{0} pi p), cos(2^{0} pi p), …, sin(2^{L-1} pi p), cos(2^{L-1} pi p)) . end{split}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;γL(p)=(sin(20πp),cos(20πp),…,sin(2L−1πp),cos(2L−1πp))​&lt;/span&gt;&lt;/span&gt; . Note that $sin(2^{L-1} pi p)$ or $cos(2^{L-1} pi p)$ has higher frequency with increasing $L$, so higher $L$ does not only contribute to higher dimensional space but also higher frequency variation on the output. As a result, positional encoding is conducive for NeRF to learn high-frequency details of a scene. To illustrate the impact of positional encoding on rendering quality, Ankur Handa has made an excellent visual comparison: . This is a great blog that provides intuitive explanation of the positional encoding used in transformers https://t.co/x8Mse71ARPUnlike RNNs transformers do not have any notion of word order so this positional encoding helps in capturing the order of words in sequence. pic.twitter.com/2IKJMamNE5 . &mdash; Ankur Handa (@ankurhandos) April 19, 2020 Now comes the hidden details - viewing direction is encoded by $ gamma^{4}(p)$ while spatial coordinate is encoded by $ gamma^{10}(p)$. This implies that features encoded from viewing direction is not so sensitive to high frequency variation compared against that from spatial coordinate. This favors the view-dependent radiance to be smooth and of low frequency with respect to the change in viewing direction. . Incorrect geometry has to be compensated by high-frequency radiance to explain well on training samples. Such low-frequency encoding on viewing direction makes NeRF struggle on high-frequency radiance, and hence guide it towards correct geometry. . 2. Viewing Direction Injected at the Near End of Network . The more layers an input feature has passed through, the more complex pattern it could expresse thanks to compounded compositions of non-linear transformations. While spatial coordinate is fed as an input at the beginning of the network, viewing direction is only appended at almost the end. . This limits the complexity that viewing direction could express as features. But on the flip side, it actually serves as a form of regularization to prevent the view-dependent radiance from going too wild. . . 3.1. NeRF Shortcoming for Modelling Unbounded Scene . . Having said NeRF comes with regularization in its structure, it is not without its shortcomings. One issue that the authors is trying to address is its limitation to model unbounded scenes. . Previous studies typically experimented NeRF on bounded scenes that are properly controlled, where an object of interest is centered at a stage and the posed views are taken at roughly a fixed distance from the object. Synthetic Dataset is one of the typical benchmark datasets in point. Such controlled environment makes NeRF easier to learn the scene, but they are far from any representations of scenes we would encounter in real life. . . In practice scenes are unbounded - there could be more than one object of interest and objects could be anywhere in the scene. In particular, background is an indispensable part of the scene so a model should preserve fidelity of both foreground and background in order to be representative of an unbounded scene. . . All these conditions impose significant challenges to standard NeRF. It is caused by the setting that during volumetric rendering standard NeRF samples 3D points from a packed volume. It works well for controlled environment because we know where the object lies, but for unbounded scenes the volume is unlikely to capture most objects from background. The consequence is a blurry background thanks to insufficient sampling of objects from background. . One remedy is to extend the length of the interval in a hope to cover most objects for sampling, but doing so has several practical limitations. If we fix the number of samples per traced ray, the rendered would look blurry in overall because of insufficient samples assigned to both foreground and background. But if we increase the number of samples, it significantly ramps up the computational cost which makes both training and rendering much more time-consuming. On top of that, it would be numerically unstable to evaluate points at far distance from the viewer. . To validate the claim, the authors have shown from experiment that standard NeRF struggles to render views of high fidelity in an unbounded Truck scene. . . 3.2. Parameterising Background as an Inverted Sphere . . The authors propose NeRF++ to address the issue. Its model structure is the same as standard NeRF, except that it segregates foreground and background of a scene into 2 separate networks: . Inner NeRF learns the foreground enclosed by a volume of unit sphere centered at origin | Outer NeRF learns the background enclosed by a volume of its complement (i.e. an inverted sphere). | . With such segregation, volumetric rendering has to take into account constitutes from both foreground and background along a traced ray in order to render colour for a pixel $ bold{C}( bold{r})$. If we denote $t’$ as the time that the traced ray crosses the boundary between the two volumes, its theoretical formulation can expressed as follows: . C(r)=∫t=0t′σ(o+td)⋅c(o+td,d)⋅e−∫s=0tσ(o+sd)dsdt+e−∫s=0t′σ(o+sd)ds∫t=t′∞σ(o+td)⋅c(o+td,d)⋅e−∫s=t′tσ(o+sd)dsdt begin{split} . bold{C}( bold{r}) &amp; = int^{t&#39;}{t=0} sigma( bold{o}+t bold{d}) cdot bold{c}( bold{o}+t bold{d}, bold{d}) cdot e^{- int^{t}{s=0} sigma( bold{o}+s bold{d})ds} dt &amp; + e^{- int^{t&#39;}{s=0} sigma( bold{o}+s bold{d})ds} int^{ infty}{t=t&#39;} sigma( bold{o}+t bold{d}) cdot bold{c}( bold{o}+t bold{d}, bold{d}) cdot e^{- int^{t}_{s=t&#39;} sigma( bold{o}+s bold{d})ds} dt . end{split}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;C(r)​=∫t=0t′​σ(o+td)⋅c(o+td,d)⋅e−∫s=0t​σ(o+sd)dsdt+e−∫s=0t′​σ(o+sd)ds∫t=t′∞​σ(o+td)⋅c(o+td,d)⋅e−∫s=t′t​σ(o+sd)dsdt​&lt;/span&gt;&lt;/span&gt; . The former term is numerical rendering of foreground and the latter term is that of background. . One caveat that deviates NeRF++ from the above formula is that outer NeRF receives different inputs than inner NeRF. Same as standard NeRF, inner NeRF is parameterized to return density $ sigma_{in}( bold{x})$ and colour $ bold{c}_{in}( bold{x}, bold{d})$ for a spatial coordinate $ bold{x}$ at viewing direction $ bold{d}$. However, outer NeRF is parameterized slightly different: instead of treating target coordinate $ bold{x}$ as input, it firsly projects $ bold{x}$ onto the unit sphere to yield coordinate $ bold{x’} = (x’, y’, z’)$. And then it takes $ bold{x’}$ and the inverse of target coordinate’s distance $1/r$ as input. As a result, its outputs are parameterized as $ sigma_{out}( bold{x’}, 1/r)$ and $ bold{c}_{out}( bold{x’}, 1/r, bold{d})$. . I personally find such parameterization clever because it skillfully normalises an unbounded spatial coordinate to bounded quantities. Such normalisation helps network to learn the background scene more efficiently. In addition, transforming unbounded inputs to bounded ones help avoid numerical unstability in optimization. . 3.3. Finding $(x’, y’, z’)$ from $1/r$ . . I think it is worth some space to explain how we could find $(x’, y’, z’)$ and $1/r$. . Let’s say we have a traced ray $ bold{r}(t) = bold{o} + t bold{d}$ marching from the inner volume through the outer volume. We start off by randomly sample $1/r$ from the interval $(0, 1)$. The value of $r$ (note that this $r$ is different from the traced ray $ bold{r}(t)$!) should determine the target coordinate $ bold{x}$ that we want: it is the point that lies on the traced ray at a distance of $r$ from the origin. With $ bold{x}$ we could therefore find its projection onto the unit sphere. . While the above intuition should work in principle, there is a simpler approach to find $(x’, y’, z’)$, as follows: . Let $ bold{b} = bold{o} + t_{b} bold{d}$ to be the vector that is perpendicular to the traced ray. Find $ bold{b}$ by solving the equation $ bold{d} cdot ( bold{o}+t_{b} bold{d}) = 0$ for $t_{b}$ | Let $ bold{a} = bold{o} + t_{a} bold{d}$ to be the vector intersecting the unit sphere and the traced ray. Find $ bold{a}$ by solving the equation $ Vert bold{o} + t_{a} bold{d} Vert = 1 $ for $t_{a}$ | Let $ theta$ be the angle between $ bold{a}$ and the traced ray. Find $ theta$ by solving $sin( theta) = Vert bold{b} Vert$ | Let $ phi$ be the angle between the target coordinate $ bold{x}$ and the traced ray. Find $ phi$ by solving $sin( phi) = Vert bold{b} Vert / r$ | Finally we get $ omega = theta - phi$. This is the clockwise rotational angle along the plane $ bold{b} times bold{a}$ that we can apply to $ bold{a}$ in order to make it aligned to the target projection $(x’, y’, z’)$! One merit of this approach is that when $r$ is updated, we only need to simply update $ phi$ and $ omega$ in order to apply another round of angular rotation on $ bold{a}$! . The geometry of this approach can be neatly visualized below. Note that I draw the geometry in 2D (instead of 3D) just for the convenience of illustration: . . 4. Experimental Results . . NeRF++ managed to render views that look sharp in both foreground and background. In comparison, background rendered by NeRF looks particularly blurry. Below are some experimental results for the scenes taken from Tanks and Temples benchmark dataset: . . The diagram below shows that the proposed parameterisation done by NeRF++ has faithfully segregated foreground from background, without sacrificing the overall fidelities of the Truck scene. . . Below are quantitative comparison on the same dataset. We can clearly notice that NeRF++ significantly outperforms NeRF for all scenes and all metrics. . As a remark, LPIPS stands for Learned Perceptual Image Patch Similarity which captures differences between image patches (lower means better) and SSIM stands for Structural Similarity, which captures perceptual similarity between 2 images. . . 5. Limitations . . Higher Computational Cost: NeRF++ is more costly on training and rendering than standard NeRF. It takes ~24 hours with 4 RTX 2080 Ti GPUs to train NeRF++ and ~30 seconds to render a single 1280x720 image. | Ignorance to Photometric Effect: Though not a issue specific to NeRF++, it doesn’t take into account unexpected photometric effects such as auto-exposure, vignetting caused by a camera. These effects may contaminate the training samples because it leads to views of different colours even at the same pose, which breaks an assumption imposed on NeRF: same colour should be rendered at same pose. | . 6. References . . NeRF++: Analyzing and Improving Neural Radiance Fields | Vladlen Koltun: Towards Photorealism (September 2020) | NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis |",
            "url": "https://riven314.github.io/alexlauwh314/paper/computer-graphics/2022/02/01/NeRF++.html",
            "relUrl": "/paper/computer-graphics/2022/02/01/NeRF++.html",
            "date": " • Feb 1, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Paper Summary: "PlenOctrees for Real-time Rendering of Neural Radiance Fields"",
            "content": ". 1. Motivations . . Neural Radiance Field (NeRF) has gained traction in academia thanks to its power to render novel 2D views of a 3D scene trained on the posed images. However NeRF’s rendering speed is noticeably slow. Without a GPU accelerator, it could easily take more than 1 minute to render a 2D view. Such drawback blocks its application on latency-sensitive domain. . It is slow becaues numerous network calls have to be made for rendering even one single pixel. The authors of this paper propose the following to address this issue and achieves a significant speedup: . Invent an efficient data structure called PlenOctrees to spatially cache the outputs from a trained NeRF | Introduce a variant of NeRF (NeRF-SH) that is easily convertible into PlenOctree representation | . 2. Introduction to Spherical Harmonics . . Spherical Harmonics (SH) is a critical concept behind their proposal. Shperical Harmonics is a collections of spherical functions used to describe the surface of some special spheres. . Technically, each of the function receives a direction in 3D space, parametrised as $( theta, phi)$, as an input and the absolute value of its output (output is a complex number) tells you the associated surface’s distance from the origin. The functions are usually annotated as $Y_{l}^{m}( theta, phi)$ where $m$ and $l$ determines the shape of the sphere to be described. . Below visualise some instances of the shperical surfaces: . . In the same sprit that some functions can be decomposed into polynomials, and that any periodic functions can be decomposed into Fourier Series, Spherical Harmonics are powerful enough to express any spherical function when composed together properly. . Additionally it provides a compact way to represent any spherical functions. We only need to book keep the coefficients for each SH function and then we can recover the target spherical function at any input (details in next session). . How could we apply this concept to NeRF? Recall that NeRF describes the geometry (i.e. density $ sigma$) and appearance (i.e. RGB colour $ bold{c}$) of any 3D point $ bold{x}$ in a 3D scene at any viewing direction $ bold{d}$. While a 3D point’s density is invariant of your viewing direction, its colour varies with your viewing angle. Therefore, we can treat the colour exactly like spherical function, except that we need 3 independent real-valued spherical functions to do so because we need 3 channels (red, green and blue) to describe a RGB colour. . 3.1. NeRF-SH: Linking Spherical Harmonics to NeRF . . The authors propose NeRF-SH that models a 3D point’s colour as spherical fucntions. NeRF-SH receives a 3D point $ bold{x}$ as input and predicts its density $ sigma$ the same way as vanilla NeRF does. But instead of predicting its RGB colour for a single viewing direction, NeRF-SH predicts spherical functions that describes the colours at all viewing direction. This modification helps factor out viewing direction from the network input. It enables a space-efficient way to spatially cache the colours predicted by the network (into a PlenOctree structure). . . We can compactly express a spherical function as coefficients of Spherical Harmonics $(k_{l}^{m})_{l:0 leq l leq l_{max}}^{m: -l leq m leq l}$, where $k_{l}^{m}$ is 3-dimensional for describing the 3 channels of RGB colour. . Once we yield the coefficients $(k_{l}^{m})_{l:0 leq l leq l_{max}}^{m: -l leq m leq l}$, we could easily render the colour at any viewing direction by a sum of Spherical Harmonics $ bold{Y}_{l}^{m}( theta, phi)$ weighted by their associated $k_{l}^{m}$, followed by a sigmoid transformation $S(.)$: . . We can neatly summarize the pipeline for training NeRF-SH with a diagram extracted from the paper. . . 3.2. NeRF-SH: Training with Sparsity Loss . . There is an additional caveat for the training: when solely supervised by standard reconstruction loss during training, NeRF-SH tends to predict arbitrary geometry on unused region. Although it doesn’t hurt the rendering quality but it occupies a lot of redundant space when NeRF-SH is converted into PlenOctree. . To encourage NeRF-SH to predict unused region to be empty, a sparsity loss is additionally enforced. To evaluate the loss, we uniformly sample $K$ points within a bounding box and consider their associated densities $ sigma_{k}$. High density leads to high sparsity loss. . . With sparsity loss enforced, the unused region can be effectively pruned away from PlenOctree thanks to their negligible densities. As a result, it leads to a tighter bound on a 3D scene and hence a higher spatial resolution on PlenOctree representation (because voxel cells are mostly distributed to the important region). . . 4.1. Conversion to PlenOctree . . Once NeRF-SH is trained, we can easily converted it into a PlenOctree representation with the following procedures: . Evaluation: Sample uniformly spaced 3D grid points and evaluate their density $ sigma$ with the trained NeRF-SH. | Filtering: Partition the grid points into different voxel cells. Render all training views and keep track of the maximum ray weight $1-exp(- sigma_{i} delta_{i})$ in each voxel cell, where $ delta_{i}$ is the distance between sample points along a ray (more details in next session). Cell with low ray weight implies it is likely an empty space with negligible contribution to any training views so we can safely prune them. | Sampling: To determine the SH coefficients for each voxel cell, we randomly sample 256 points within a voxel cells and take an average of their associated SH coefficients. | 4.2. Rendering with PlenOctree . . With the converted PlenOctree, we could easily achieve blazingly fast rendering speed with the following procedures: . Cast a ray $ bold{r}$ from the pixel to be rendered into the PlenOctree and consider all voxel cells that intersect with the ray. | Segment the ray by voxel boundaries. Consider the lengths of each segments as ${ delta_{i}}_{i=1}^{N}$ and look up their associated density ${ sigma_{i} }_{i=1}^{N}$ and SH coefficients ${ bold{k_{i}} }_{i=1}^{N}$ from each voxel cell. Colour of each segment ${ c_{i} }_{i=1}^N$ can be found with the spherical functions represented by the SH coefficients. | With the above quantities, apply standard volumetric rendering formula to render colour $ hat{C}( bold{r})$ at the target pixel. | We could achieve further speedup by early stopping the ray when its accumulated transmittance $T_{i}$ is too low. The accumulated transmittance indicates the chance that the ray can pass through the first segment up to the target segments without being blocked. Points with low accumulated transmittance have negligible impact to the rendered colour so we could safely skip them. | . The whole process is similar to that of NeRF except PlenOctree makes use of cache from its voxel cells so it can achieve significant speedup comparing against a standard NeRF. . We can summarise the rendering process with a diagram below. $Y(.)$ is the SH functions as a vector and $ bold{k}_{i}$ are the SH coefficients associated to the $i$-th voxel cell. $c_{i}$ is its associated colour and the formula is no different from what we discussed before. . . 4.3. Fine-tuning PlenOctree . . Since the rendering process with PlenOctree is done by standard volumetric rendering, the operation is differentiable. Therefore, we could apply stochastic gradient descent to fine-tune the PlenOctree representation. Empirical studies show that additional fine-tuning on PlenOctree could lead to significant improvement on rendering quality. . While in principle it is feasible to train a PlenOctree representation from scratch, it usually takes much more time to converge. A trained NeRF-SH gives PlenOctree a good prior of geometry and appearance to learn from. . We can neatly summarised the pipeline for yielding a PlenOctree representation with the following diagram extracted from the paper. . . 5. Experimental Results . . The authors compares both the rendering speed and quality between their approach (NeRF-SH + PlenOctree) and existing models. The rendering speed is measured by Frames per Second (FPS) and the rendering quality is measured by Peak Signal-to-Noise Ratio (PSNR). . They experimented their approach with different settings (e.g. higher filtering threshold on accumulated transmittance, reduce grid size to 256) and found that a few of its settings (i.e. Ours-1.9G) achieved better PSNR and 3000x faster FPS against vanilla NeRF! . . 6. Limitations . . While NeRF-SH and PlenOctree achieves amazing rendering speed with comparable rendering quality against standard NeRF, it has several trade-offs. . On one hand, it has larger memory footprint than a standard NeRF. A NeRF model is typically light-weighted with roughly ~10 MB. However, a PlenOctree representation could easily take up to ~2 GB because it caches density and SH coefficients in each voxel cell. . On the other hand, it creates noticeable artifact when you zoom in the scene because it partitions a continuous 3D space into discrete voxel cells. The resolution is inevitably sacraficed. . . 7. References . . PlenOctrees for Real-time Rendering of Neural Radiance Fields | Real-time rendering of NeRFs with PlenOctrees - Angjoo Kanazawa |",
            "url": "https://riven314.github.io/alexlauwh314/paper/computer-graphics/2022/01/26/PlenOctree-Summary.html",
            "relUrl": "/paper/computer-graphics/2022/01/26/PlenOctree-Summary.html",
            "date": " • Jan 26, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://riven314.github.io/alexlauwh314/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://riven314.github.io/alexlauwh314/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://riven314.github.io/alexlauwh314/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}